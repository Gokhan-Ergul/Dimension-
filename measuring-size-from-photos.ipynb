{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11114792,"sourceType":"datasetVersion","datasetId":6930097},{"sourceId":11114969,"sourceType":"datasetVersion","datasetId":6930226},{"sourceId":355510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":296515,"modelId":317123},{"sourceId":357095,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":297637,"modelId":318241}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"As you will see below, the dataset is not very consistent, and since there is no data from a device like a Ridar device that measures distance, I am conducting this work using images of over 25,000 products shared in the review sections of selected products on an e-commerce site, with the goal of answering the question: Can the dimensions of products be estimated solely by taking pictures from different angles?\n\n**My aim is not to achieve a successful result, but to answer the question of how accurately the dimensions can be predicted.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:11:30.127052Z","iopub.execute_input":"2025-04-25T17:11:30.127297Z","iopub.status.idle":"2025-04-25T17:11:55.653099Z","shell.execute_reply.started":"2025-04-25T17:11:30.127276Z","shell.execute_reply":"2025-04-25T17:11:55.65221Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch import nn, optim\nfrom PIL import Image\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom PIL import Image\nimport random\nimport warnings\nimport re\nimport matplotlib.pyplot as plt\nwarnings.simplefilter(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:18:47.326037Z","iopub.execute_input":"2025-04-25T17:18:47.326422Z","iopub.status.idle":"2025-04-25T17:18:54.668915Z","shell.execute_reply.started":"2025-04-25T17:18:47.326394Z","shell.execute_reply":"2025-04-25T17:18:54.668228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_folder_paths = '/kaggle/input/dimension-photos'\nimages_name=os.listdir(images_folder_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:18:56.362132Z","iopub.execute_input":"2025-04-25T17:18:56.362562Z","iopub.status.idle":"2025-04-25T17:18:56.396908Z","shell.execute_reply.started":"2025-04-25T17:18:56.362537Z","shell.execute_reply":"2025-04-25T17:18:56.396308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(images_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:18:57.386032Z","iopub.execute_input":"2025-04-25T17:18:57.386368Z","iopub.status.idle":"2025-04-25T17:18:57.394338Z","shell.execute_reply.started":"2025-04-25T17:18:57.386343Z","shell.execute_reply":"2025-04-25T17:18:57.393467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_name[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:18:58.307674Z","iopub.execute_input":"2025-04-25T17:18:58.307954Z","iopub.status.idle":"2025-04-25T17:18:58.31301Z","shell.execute_reply.started":"2025-04-25T17:18:58.307932Z","shell.execute_reply":"2025-04-25T17:18:58.312167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[name for name in images_name if 'English_Home' in name] # On average there are 60 photos of each product","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:18:58.798465Z","iopub.execute_input":"2025-04-25T17:18:58.798752Z","iopub.status.idle":"2025-04-25T17:18:58.806761Z","shell.execute_reply.started":"2025-04-25T17:18:58.798725Z","shell.execute_reply":"2025-04-25T17:18:58.806059Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths = []\nfor image_name in images_name:\n    full_path = os.path.join(images_folder_paths,image_name)\n    full_photo_paths.append(full_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:00.697842Z","iopub.execute_input":"2025-04-25T17:19:00.698171Z","iopub.status.idle":"2025-04-25T17:19:00.735273Z","shell.execute_reply.started":"2025-04-25T17:19:00.698141Z","shell.execute_reply":"2025-04-25T17:19:00.734607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:01.025051Z","iopub.execute_input":"2025-04-25T17:19:01.025323Z","iopub.status.idle":"2025-04-25T17:19:01.03014Z","shell.execute_reply.started":"2025-04-25T17:19:01.025301Z","shell.execute_reply":"2025-04-25T17:19:01.029466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Image.open(full_photo_paths[100])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:02.696736Z","iopub.execute_input":"2025-04-25T17:19:02.69715Z","iopub.status.idle":"2025-04-25T17:19:02.785586Z","shell.execute_reply.started":"2025-04-25T17:19:02.697113Z","shell.execute_reply":"2025-04-25T17:19:02.784745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv = pd.read_csv('/kaggle/input/dimension-csv/finaldata.csv')\ndf_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:03.230899Z","iopub.execute_input":"2025-04-25T17:19:03.231256Z","iopub.status.idle":"2025-04-25T17:19:03.409936Z","shell.execute_reply.started":"2025-04-25T17:19:03.231227Z","shell.execute_reply":"2025-04-25T17:19:03.4092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv['product_name'] = df_csv['product_name'].str.replace('-', '_')\ndf_csv['product_name'] = df_csv['product_name'].str.replace(',', '')\ndf_csv['product_name'] = df_csv['product_name'].str.replace('(','')\ndf_csv['product_name'] = df_csv['product_name'].str.replace(')','')\ndf_csv['product_name'] = df_csv['product_name'].str.replace('™','')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:04.978827Z","iopub.execute_input":"2025-04-25T17:19:04.979146Z","iopub.status.idle":"2025-04-25T17:19:05.018829Z","shell.execute_reply.started":"2025-04-25T17:19:04.97911Z","shell.execute_reply":"2025-04-25T17:19:05.018012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:05.434526Z","iopub.execute_input":"2025-04-25T17:19:05.434798Z","iopub.status.idle":"2025-04-25T17:19:05.476737Z","shell.execute_reply.started":"2025-04-25T17:19:05.434776Z","shell.execute_reply":"2025-04-25T17:19:05.476045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_name[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:06.198729Z","iopub.execute_input":"2025-04-25T17:19:06.199055Z","iopub.status.idle":"2025-04-25T17:19:06.204186Z","shell.execute_reply.started":"2025-04-25T17:19:06.199017Z","shell.execute_reply":"2025-04-25T17:19:06.203413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:06.790338Z","iopub.execute_input":"2025-04-25T17:19:06.790615Z","iopub.status.idle":"2025-04-25T17:19:06.795523Z","shell.execute_reply.started":"2025-04-25T17:19:06.790593Z","shell.execute_reply":"2025-04-25T17:19:06.794829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:07.306871Z","iopub.execute_input":"2025-04-25T17:19:07.307204Z","iopub.status.idle":"2025-04-25T17:19:07.316522Z","shell.execute_reply.started":"2025-04-25T17:19:07.307175Z","shell.execute_reply":"2025-04-25T17:19:07.315581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_only_name = []\nfor i in images_name:\n    images_only_name.append(i.rsplit('.', 1)[0])\n\nimages_only_name[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:08.042501Z","iopub.execute_input":"2025-04-25T17:19:08.042845Z","iopub.status.idle":"2025-04-25T17:19:08.060525Z","shell.execute_reply.started":"2025-04-25T17:19:08.042813Z","shell.execute_reply":"2025-04-25T17:19:08.059838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = ['Id','product_name','height','width','image_path']\ndf = pd.DataFrame(columns=keys)\n\nfor inx, i  in enumerate(df_csv['product_name']):\n    ID = df_csv['Id'].iloc[inx]\n    result = f\"{i}_{ID}\"\n    if result in images_only_name:\n        img_index = images_only_name.index(result)\n        df.loc[len(df)]=[df_csv['Id'].iloc[inx],df_csv['product_name'].iloc[inx], df_csv['height'].iloc[inx],df_csv['width'].iloc[inx],full_photo_paths[img_index]]\n\n    else:\n        print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:08.58554Z","iopub.execute_input":"2025-04-25T17:19:08.585833Z","iopub.status.idle":"2025-04-25T17:19:49.871578Z","shell.execute_reply.started":"2025-04-25T17:19:08.585806Z","shell.execute_reply":"2025-04-25T17:19:49.8709Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:49.872562Z","iopub.execute_input":"2025-04-25T17:19:49.872759Z","iopub.status.idle":"2025-04-25T17:19:49.881531Z","shell.execute_reply.started":"2025-04-25T17:19:49.872741Z","shell.execute_reply":"2025-04-25T17:19:49.880836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"product_name_list = []\nfor img_path in df['image_path']:\n    result1 = img_path.split('/')[4]\n    result2 = result1.rsplit('_', 1)[0]\n    product_name_list.append(result2)\n\nunique_listt = list(set(product_name_list))\nprint(len(unique_listt)) #i have 511 different product.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:49.882835Z","iopub.execute_input":"2025-04-25T17:19:49.883109Z","iopub.status.idle":"2025-04-25T17:19:49.925542Z","shell.execute_reply.started":"2025-04-25T17:19:49.883063Z","shell.execute_reply":"2025-04-25T17:19:49.924669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_listt[:5] #some examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:49.926809Z","iopub.execute_input":"2025-04-25T17:19:49.927102Z","iopub.status.idle":"2025-04-25T17:19:49.943538Z","shell.execute_reply.started":"2025-04-25T17:19:49.927056Z","shell.execute_reply":"2025-04-25T17:19:49.942675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_some_photos(product_name = '.', already_list = []):\n    images = list()\n    vertical_images=[]\n    counter = 0\n    gal_list = [name for name in full_photo_paths if product_name in name]\n    if len(already_list)>0:\n        gal_list = already_list\n        \n    for path in gal_list:\n        image = Image.open(path)\n        images.append(image)            \n        \n    fig, axes = plt.subplots(len(images)//5, len(images) //(len(images)//5), figsize=(15, len(images)//1.5))\n    for ax, img in zip(axes.ravel(), images):\n        ax.imshow(img)\n        ax.axis(\"off\")\n    plt.suptitle(f\"{str(len(images))} samples of {product_name} of the dataset\", fontsize=16, y=0.9)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:49.944407Z","iopub.execute_input":"2025-04-25T17:19:49.944699Z","iopub.status.idle":"2025-04-25T17:19:49.959566Z","shell.execute_reply.started":"2025-04-25T17:19:49.944671Z","shell.execute_reply":"2025-04-25T17:19:49.958755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_some_photos('Rota_Hediyelik')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:47:57.685286Z","iopub.status.idle":"2025-04-25T08:47:57.685623Z","shell.execute_reply":"2025-04-25T08:47:57.685503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_some_photos('B101_LED_Işık_Micro_USB_Type_C_Girişli_10.000_mAh_Taşınabilir_Şarj_Cihazı_Powerbank_Gri')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:47:57.686229Z","iopub.status.idle":"2025-04-25T08:47:57.686556Z","shell.execute_reply":"2025-04-25T08:47:57.686434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ****As you can see some pictures are irrelevant\n### so I am going to use VGG16 to determine these","metadata":{}},{"cell_type":"markdown","source":"## How does Keras handle resizing?\nKeras' image.load_img(img_path, target_size=(224, 224)) uses nearest neighbor interpolation (or another interpolation method) to directly stretch or shrink the image to the given size. This means:\n\nIf the image is larger, it will shrink.\n\nIf the image is smaller, it will expand.\n\nThe aspect ratio will not be preserved (it will distort the image instead of adding black bars).\n\n**So at first I need to convert the images to 224x224 so I'm going to use padding because I want to keep the ratio of photos**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimage_sizes = []\n\n# Tüm görüntülerin boyutlarını topla\nfor path in full_photo_paths:\n    img = Image.open(path)\n    image_sizes.append(img.size) # (width, height)\n\nCounter(image_sizes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:53:41.247211Z","iopub.execute_input":"2025-04-25T08:53:41.247515Z","iopub.status.idle":"2025-04-25T08:56:02.775241Z","shell.execute_reply.started":"2025-04-25T08:53:41.247489Z","shell.execute_reply":"2025-04-25T08:56:02.774224Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Image.open(full_photo_paths[50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:02.77668Z","iopub.execute_input":"2025-04-25T08:56:02.777053Z","iopub.status.idle":"2025-04-25T08:56:02.786656Z","shell.execute_reply.started":"2025-04-25T08:56:02.777019Z","shell.execute_reply":"2025-04-25T08:56:02.785692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counter = 0\nfor i in range(len(full_photo_paths)):\n    img = Image.open(full_photo_paths[i])\n    if img.size == (140, 311):\n        Image.open(full_photo_paths[i])\n        # I cant use img.show() beacuse of Kaggle\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.show()\n        print(f\"index: {i}\")\n        counter+=1\n        if counter %5==0:\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:02.788294Z","iopub.execute_input":"2025-04-25T08:56:02.788585Z","iopub.status.idle":"2025-04-25T08:56:03.480398Z","shell.execute_reply.started":"2025-04-25T08:56:02.78856Z","shell.execute_reply":"2025-04-25T08:56:03.479147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\ndef resize_with_padding(image_paths, target_size=(224, 224)):\n    padded_images = []\n    \n    for img_path in image_paths:\n        # Load the image\n        image = cv2.imread(img_path)\n        if image is None:\n            raise ValueError(f\"Image at {img_path} could not be loaded.\")\n        \n        old_size = image.shape[:2]  # (height, width)\n        ratio = min(target_size[0] / old_size[0], target_size[1] / old_size[1])\n\n        new_size = (int(old_size[1] * ratio), int(old_size[0] * ratio))  # (width, height)\n        resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n\n        # Padding calculation\n        delta_w = target_size[1] - new_size[0]\n        delta_h = target_size[0] - new_size[1]\n\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n\n        padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n\n        padded_images.append({'image': padded_image, 'path': img_path})\n    \n    return padded_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:49.96041Z","iopub.execute_input":"2025-04-25T17:19:49.960601Z","iopub.status.idle":"2025-04-25T17:19:50.602333Z","shell.execute_reply.started":"2025-04-25T17:19:49.960585Z","shell.execute_reply":"2025-04-25T17:19:50.601677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(full_photo_paths[96])\nplt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:03.81609Z","iopub.execute_input":"2025-04-25T08:56:03.816388Z","iopub.status.idle":"2025-04-25T08:56:04.04164Z","shell.execute_reply.started":"2025-04-25T08:56:03.816363Z","shell.execute_reply":"2025-04-25T08:56:04.040643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" cv2.imread(full_photo_paths[96]).shape[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.603021Z","iopub.execute_input":"2025-04-25T17:19:50.603291Z","iopub.status.idle":"2025-04-25T17:19:50.644621Z","shell.execute_reply.started":"2025-04-25T17:19:50.603271Z","shell.execute_reply":"2025-04-25T17:19:50.643902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"padded_images = resize_with_padding([full_photo_paths[96]], target_size=(224, 224))\n\n# Display the resized images using matplotlib\nfor i, img in enumerate(padded_images):\n    plt.subplot(1, len(padded_images), i + 1)\n    img_rgb = cv2.cvtColor(img['image'], cv2.COLOR_BGR2RGB)\n    plt.imshow(img_rgb)\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.646677Z","iopub.execute_input":"2025-04-25T17:19:50.646875Z","iopub.status.idle":"2025-04-25T17:19:50.815872Z","shell.execute_reply.started":"2025-04-25T17:19:50.646857Z","shell.execute_reply":"2025-04-25T17:19:50.814963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Detection","metadata":{}},{"cell_type":"markdown","source":"## Visual Outlier Detection with VGG16","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\ndef extract_features(img_path, model, device='cpu'):\n    img = Image.open(img_path).convert('RGB')\n\n    # Define preprocessing pipeline\n    preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n    ])\n    img_tensor = preprocess(img)\n    img_tensor = img_tensor.unsqueeze(0) # to add batch \n\n    img_tensor = img_tensor.to(device)\n    \n    model.eval() #extract features\n    with torch.no_grad(): #avoid waste of memory and something like that\n        features = model(img_tensor)\n    features = features.cpu().numpy().flatten()\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:04.172288Z","iopub.execute_input":"2025-04-25T08:56:04.172648Z","iopub.status.idle":"2025-04-25T08:56:04.178872Z","shell.execute_reply.started":"2025-04-25T08:56:04.172618Z","shell.execute_reply":"2025-04-25T08:56:04.177925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom sklearn.ensemble import IsolationForest\n\nmodel = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nmodel.avgpool = nn.AdaptiveAvgPool2d(1)  # Global average pooling layer\nmodel.classifier = nn.Identity()  # Remove the classification layer (optional)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\nproduct_name = 'Beyaz_Kupa_Bardak_Winnie_Arkadaşlık_Dostluk_Kardeşli'\nimage_files = [name for name in full_photo_paths if product_name in name]\n\n# Extract features for each image\nfeature_vectors = np.array([extract_features(img, model) for img in image_files])\n\n#PCA for dimensionality reduction:\npca = PCA(n_components=10)  \nreduced_features = pca.fit_transform(feature_vectors)\n\n# Cosine similarity hesaplama\nsimilarity_matrix = cosine_similarity(reduced_features)\n\n# Outlier tespiti için Isolation Forest kullanma\niso_forest = IsolationForest(contamination=0.1)  # Kontaminasyon oranını ayarlayabilirsin (0.1, %10'luk dilim)\noutlier_predictions = iso_forest.fit_predict(reduced_features)\n\n# Outlier fotoğrafları bulma\noutlier_indices = np.where(outlier_predictions == -1)[0]  # -1 outlier anlamına gelir\noutlier_images = [image_files[i] for i in outlier_indices]\n\nprint(\"Outlier (kupa içermeyen) resimler:\", outlier_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:04.180001Z","iopub.execute_input":"2025-04-25T08:56:04.180354Z","iopub.status.idle":"2025-04-25T08:56:28.383905Z","shell.execute_reply.started":"2025-04-25T08:56:04.180287Z","shell.execute_reply":"2025-04-25T08:56:28.382953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_some_photos(';',outlier_images) #but it isint work as i expected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:56:28.385178Z","iopub.execute_input":"2025-04-25T08:56:28.385772Z","iopub.status.idle":"2025-04-25T08:56:28.698426Z","shell.execute_reply.started":"2025-04-25T08:56:28.385731Z","shell.execute_reply":"2025-04-25T08:56:28.697137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If I crop the image to show only the object after detecting it in an image with yolo, the model cannot learn the auxiliary objects around that object (hand, ruler, etc. if there is one), but if I do not delete it, the model will be trained according to the objects around it instead of the object enclosed in a rectangle (the actual desired object), this time it will be an error. What should I do at this stage?","metadata":{}},{"cell_type":"markdown","source":"My decision is this: I need to crop because the backgrounds are so different and I have too many class (511) and every class has only 60 photos","metadata":{}},{"cell_type":"markdown","source":"## YOLO\n### YOLO (You Only Look Once) is a state-of-the-art object detection algorithm known for its speed and efficiency. It processes an entire image in a single forward pass of the network, predicting both the class and bounding box coordinates for multiple objects simultaneously.\n\n### YOLO models are pre-trained on datasets like COCO and can detect dozens of object classes in real-time. However, for detecting custom classes, the model must be fine-tuned or retrained on labeled data. While not as flexible as CLIP+SAM for zero-shot tasks, YOLO remains a top choice for real-time applications such as surveillance, autonomous driving, and robotics due to its high accuracy and low latency.\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n\n!pip install ultralytics\nclear_output()\n\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolov8x.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:05:35.656654Z","iopub.execute_input":"2025-04-25T09:05:35.657021Z","iopub.status.idle":"2025-04-25T09:05:40.062783Z","shell.execute_reply.started":"2025-04-25T09:05:35.656994Z","shell.execute_reply":"2025-04-25T09:05:40.06182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['bounding_box_width'] = 0.0\ndf['bounding_box_height'] = 0.0\ndf['bounding_box_ratio'] = 0.0 #bounding_box_width / bounding_box_height\ndf['box_relative_width'] = 0.0 #bounding_box_width / image_width — görselde ne kadar yer kapladığı.\ndf['box_relative_height'] = 0.0 #bounding_box_height / image_height — yükseklik bazında kaplama\ndf['confidence'] = 0.0 #Confidence score given by YOLO.\ndf['image_area'] = 0.0 #\timage_width * image_height\ndf['box_area'] = 0.0\ndf['box_center_x'] = 0.0\ndf['box_center_y'] = 0.0\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.817591Z","iopub.execute_input":"2025-04-25T17:19:50.817926Z","iopub.status.idle":"2025-04-25T17:19:50.841692Z","shell.execute_reply.started":"2025-04-25T17:19:50.817891Z","shell.execute_reply":"2025-04-25T17:19:50.840828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_and_plot_from_ndarray_list(image_list, model):\n    records = []\n    for idx, image in enumerate(image_list):\n        if image is None or not isinstance(image['image'], np.ndarray):\n            print(f\"LList element {idx} is not a valid image, skipping.\")\n            continue\n            \n        temp_path = f\"temp_padded_{idx}.jpg\"\n        cv2.imwrite(temp_path, image['image'])\n\n        results = model(temp_path)[0]\n        img = cv2.cvtColor(image['image'], cv2.COLOR_BGR2RGB)\n\n        plt.figure(figsize=(6, 6))\n        plt.imshow(img)\n        plt.axis('off')\n        image_h, image_w = img.shape[:2]\n\n        if results.boxes:\n            best_box = max(results.boxes, key=lambda b: b.conf[0].item())\n\n            x1, y1, x2, y2 = best_box.xyxy[0].cpu().numpy()\n            cls = int(best_box.cls[0].item())\n            conf = float(best_box.conf[0].item())\n\n            box_width = x2 - x1\n            box_height = y2 - y1\n            box_area = box_width * box_height\n            box_center_x = x1 + box_width / 2\n            box_center_y = y1 + box_height / 2\n\n            bounding_box_ratio = box_width / box_height if box_height != 0 else 0\n            box_relative_width = box_width / image_w\n            box_relative_height = box_height / image_h\n            image_area = image_h * image_w\n\n            print('--------------------------------------------------------')\n            img_id = int(re.search(r'_(\\d+)\\.jpg', image['path']).group(1))\n            print(re.search(r'_(\\d+)\\.jpg', image['path']).group(1))\n            df.loc[df['Id']== img_id, ['bounding_box_width',\n                                       'bounding_box_height', \n                                       'bounding_box_ratio',\n                                       'box_relative_width', \n                                       'box_relative_height',\n                                       'confidence',\n                                       'image_area', \n                                       'box_area', \n                                       'box_center_x', 'box_center_y']] = [\n                                                                           box_width, \n                                                                           box_height, \n                                                                           bounding_box_ratio,   \n                                                                           box_relative_width, \n                                                                           box_relative_height, \n                                                                           conf, \n                                                                           image_area, \n                                                                           box_area, \n                                                                           box_center_x, \n                                                                           box_center_y]\n\n            \n            plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                              edgecolor='lime', facecolor='none', linewidth=2))\n            \n        else:\n            print(f\"⚠️ No box detected in image {idx}.\")\n\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T14:48:29.782207Z","iopub.execute_input":"2025-04-25T14:48:29.782615Z","iopub.status.idle":"2025-04-25T14:48:29.797798Z","shell.execute_reply.started":"2025-04-25T14:48:29.782581Z","shell.execute_reply":"2025-04-25T14:48:29.796771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = [name for name in full_photo_paths if 'Cep_Çanta_Aynas' in name][:5]\n\npadded_images = resize_with_padding(img_path, target_size=(224, 224))\ndetect_and_plot_from_ndarray_list(padded_images, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:15:18.776029Z","iopub.execute_input":"2025-04-25T09:15:18.776434Z","iopub.status.idle":"2025-04-25T09:15:31.820169Z","shell.execute_reply.started":"2025-04-25T09:15:18.776399Z","shell.execute_reply":"2025-04-25T09:15:31.819194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CLIP + SAM\n### CLIP (Contrastive Language–Image Pretraining) and SAM (Segment Anything Model) can be combined to enable a more flexible and language-guided form of object detection. CLIP maps both images and text into a shared embedding space, allowing users to specify objects using natural language (e.g., \"a person with a red hat\"). SAM, on the other hand, excels at segmenting objects at the pixel level with high precision.\n\n### When used together, CLIP identifies which parts of the image are semantically similar to the text prompt, while SAM precisely segments those regions. This combination is particularly powerful in zero-shot scenarios where no training on the specific object class is required. It’s ideal for interactive tools, content search, or dataset creation where labeled data is limited.\n\n","metadata":{}},{"cell_type":"markdown","source":"to use CLIP I need to rewrite the df['product_name'] in enligsh ","metadata":{}},{"cell_type":"code","source":"df.loc[df['product_name']=='glass_sphere', ['product_name']] = 'glass light emitting sphere'\ndf.loc[df['product_name']=='Potted_Cactu_Plush_Toy', ['product_name']] = 'Toy cactus'\ndf.loc[df['product_name']=='Tarak', ['product_name']] = 'black women comb'\n\n\ndf.loc[df['product_name']=='torch', ['product_name']] = 'flashlight in green box'\ndf.loc[df['product_name']=='glove2', ['product_name']] = 'white cloth'\ndf.loc[df['product_name']=='parfumm', ['product_name']] = 'perfume'\ndf.loc[df['product_name']=='Genel_Markalar', ['product_name']] = 'plastic  light emitting sphere'\ndf.loc[df['product_name']=='Orkide_Yetiştirme', ['product_name']] = 'black seed'\ndf.loc[df['product_name']=='BEYZANA', ['product_name']] = 'pencil bag'\ndf.loc[df['product_name']=='DEMPOWER', ['product_name']] = 'white small tool'\n\ndf.loc[df['product_name']=='Midilli', ['product_name']] = 'pink notebook'\ndf.loc[df['product_name']=='Tonny_Black', ['product_name']] = 'black leather wallet'\ndf.loc[df['product_name']=='LET_SCRUB', ['product_name']] = 'white cloth'\ndf.loc[df['product_name']=='woys', ['product_name']] = 'black handbag'\ndf.loc[df['product_name']=='Dekals', ['product_name']] = 'pink buckle'\ndf.loc[df['product_name']=='EMBHOME', ['product_name']] = 'steel ring iron'\ndf.loc[df['product_name']=='Paşabahçe', ['product_name']] = 'rectangular glass'\ndf.loc[df['product_name']=='Küçük_El_Feneri_4_Adet', ['product_name']] = 'plastic flashlights'\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.842485Z","iopub.execute_input":"2025-04-25T17:19:50.842758Z","iopub.status.idle":"2025-04-25T17:19:50.907771Z","shell.execute_reply.started":"2025-04-25T17:19:50.84273Z","shell.execute_reply":"2025-04-25T17:19:50.906931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = [name for name in full_photo_paths if 'Genel_Markalar' in name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.908471Z","iopub.execute_input":"2025-04-25T17:19:50.908677Z","iopub.status.idle":"2025-04-25T17:19:50.91483Z","shell.execute_reply.started":"2025-04-25T17:19:50.908659Z","shell.execute_reply":"2025-04-25T17:19:50.913982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['product_name'] == 'rectangular glass']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:50.9155Z","iopub.execute_input":"2025-04-25T17:19:50.915677Z","iopub.status.idle":"2025-04-25T17:19:50.98307Z","shell.execute_reply.started":"2025-04-25T17:19:50.915662Z","shell.execute_reply":"2025-04-25T17:19:50.982262Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n!pip install opencv-python matplotlib\n!pip install git+https://github.com/facebookresearch/segment-anything.git\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:23:20.253555Z","iopub.execute_input":"2025-04-25T09:23:20.253939Z","iopub.status.idle":"2025-04-25T09:23:39.913555Z","shell.execute_reply.started":"2025-04-25T09:23:20.253906Z","shell.execute_reply":"2025-04-25T09:23:39.912258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"padded_images = resize_with_padding(img_path, target_size=(224, 224))\ntype(padded_images[0]['image'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:25:14.981366Z","iopub.execute_input":"2025-04-25T09:25:14.981777Z","iopub.status.idle":"2025-04-25T09:25:15.090206Z","shell.execute_reply.started":"2025-04-25T09:25:14.981742Z","shell.execute_reply":"2025-04-25T09:25:15.089349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_id_from_filename(filename):\n    # lamba_23.jpg -> 23\n    return int(filename.split(\"_\")[-1].split(\".\")[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:19:54.405185Z","iopub.execute_input":"2025-04-25T17:19:54.405491Z","iopub.status.idle":"2025-04-25T17:19:54.409554Z","shell.execute_reply.started":"2025-04-25T17:19:54.405467Z","shell.execute_reply":"2025-04-25T17:19:54.408676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from segment_anything import sam_model_registry, SamPredictor\nimport clip\nfrom tqdm import tqdm\n\n\nimages_path = \"/kaggle/input/dimension-photos\" \nsam_checkpoint = \"/kaggle/input/sam_model/pytorch/default/1/sam_vit_h_4b8939.pth\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# CLIP\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# SAM\nsam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint).to(device)\npredictor = SamPredictor(sam)\n\n\ndef run_clip_sam(image_path, prompt):\n    image = Image.open(image_path).convert(\"RGB\")\n\n    padded_image = resize_with_padding([image_path], target_size=(224, 224))\n    image_np = np.array(padded_image[0]['image'])\n    image_tensor = clip_preprocess(image).unsqueeze(0).to(device)\n\n    # CLIP: prompt and the picture\n    with torch.no_grad():\n        text = clip.tokenize([prompt]).to(device)\n        image_features = clip_model.encode_image(image_tensor)\n        text_features = clip_model.encode_text(text)\n        similarity = torch.cosine_similarity(image_features, text_features).item()\n\n    # SAM segmentation\n    predictor.set_image(image_np)\n    masks, scores, _ = predictor.predict(box=None, multimask_output=True)\n\n    if masks is None or len(masks) == 0:\n        return None\n\n    best_mask = masks[np.argmax(scores)]\n    ys, xs = np.where(best_mask)\n    if len(xs) == 0 or len(ys) == 0:\n        return None\n    x_min, x_max, y_min, y_max = xs.min(), xs.max(), ys.min(), ys.max()\n\n    # Normalize (YOLO formatı)\n    h, w = image_np.shape[:2]\n    x_center = ((x_min + x_max) / 2) / w\n    y_center = ((y_min + y_max) / 2) / h\n    box_w = (x_max - x_min) / w\n    box_h = (y_max - y_min) / h\n\n    return [x_center, y_center, box_w, box_h]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T13:42:12.932329Z","iopub.execute_input":"2025-04-25T13:42:12.932722Z","iopub.status.idle":"2025-04-25T13:42:59.875994Z","shell.execute_reply.started":"2025-04-25T13:42:12.932677Z","shell.execute_reply":"2025-04-25T13:42:59.874888Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_box_on_image(image_path, box):\n    padded_image = resize_with_padding([image_path], target_size=(224, 224))\n    image = padded_image[0]['image']\n    \n    h, w = image.shape[:2]\n\n    x_center, y_center, box_w, box_h = box\n    x_min = int((x_center - box_w / 2) * w)\n    x_max = int((x_center + box_w / 2) * w)\n    y_min = int((y_center - box_h / 2) * h)\n    y_max = int((y_center + box_h / 2) * h)\n\n    # Kutuyu çiz\n    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n\n    plt.figure(figsize=(6, 6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:26:19.78394Z","iopub.execute_input":"2025-04-25T09:26:19.784466Z","iopub.status.idle":"2025-04-25T09:26:19.793061Z","shell.execute_reply.started":"2025-04-25T09:26:19.784432Z","shell.execute_reply":"2025-04-25T09:26:19.791926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:28:54.78553Z","iopub.execute_input":"2025-04-25T09:28:54.785917Z","iopub.status.idle":"2025-04-25T09:28:54.79235Z","shell.execute_reply.started":"2025-04-25T09:28:54.785885Z","shell.execute_reply":"2025-04-25T09:28:54.791264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['product_name'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:20:21.331342Z","iopub.execute_input":"2025-04-25T16:20:21.331677Z","iopub.status.idle":"2025-04-25T16:20:21.345169Z","shell.execute_reply.started":"2025-04-25T16:20:21.331649Z","shell.execute_reply":"2025-04-25T16:20:21.344064Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, image_file in enumerate(sorted(img_path[:10])):\n    image_id = extract_id_from_filename(image_file)\n    row = df[df[\"Id\"] == image_id].iloc[0]\n    product_name = row[\"product_name\"]\n    image_path_full = os.path.join(images_path, image_file)\n    box = run_clip_sam(image_path_full, prompt=product_name)\n    if box:\n        print(f\"Image: {image_file} | Prompt: {product_name}\")\n        draw_box_on_image(image_path_full, box)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:32:44.766212Z","iopub.execute_input":"2025-04-25T09:32:44.766612Z","iopub.status.idle":"2025-04-25T09:40:00.915428Z","shell.execute_reply.started":"2025-04-25T09:32:44.766582Z","shell.execute_reply":"2025-04-25T09:40:00.914018Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it put in a boxt all of the photos","metadata":{}},{"cell_type":"markdown","source":"## GroundingDINO","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(\"GroundingDINO\"):\n    !git clone https://github.com/IDEA-Research/GroundingDINO.git\n\n%cd GroundingDINO\n!pip install -e .\n!pip install -q groundingdino transformers torchvision matplotlib\n!pip install -q -e GroundingDINO\n!pip install -q git+https://github.com/openai/CLIP.git\nfrom IPython.display import clear_output\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:21:59.4929Z","iopub.execute_input":"2025-04-25T17:21:59.493271Z","iopub.status.idle":"2025-04-25T17:23:02.802201Z","shell.execute_reply.started":"2025-04-25T17:21:59.493232Z","shell.execute_reply":"2025-04-25T17:23:02.801279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport os\n\nmodel_url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/0.1.0/groundingdino_swint_ogc.pth\"\nmodel_path = \"/kaggle/input/groundingdino_model/pytorch/default/1/groundingdino_swint_ogc.pth\"\n\nif not os.path.exists(model_path):\n    print(\"📥 Model downloading with stream...\")\n    with requests.get(model_url, stream=True) as r:\n        r.raise_for_status()\n        with open(model_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n    print(\"✅ Model downloaded.\")\nelse:\n    print(\"✅ Model already available.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:23:16.635709Z","iopub.execute_input":"2025-04-25T17:23:16.636031Z","iopub.status.idle":"2025-04-25T17:23:16.642655Z","shell.execute_reply.started":"2025-04-25T17:23:16.636007Z","shell.execute_reply":"2025-04-25T17:23:16.641974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from groundingdino.util.inference import load_model, load_image, predict, annotate\n\n# Load the Model\nconfig_path = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\nmodel = load_model(config_path, model_path)\nmodel.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:23:18.17585Z","iopub.execute_input":"2025-04-25T17:23:18.176211Z","iopub.status.idle":"2025-04-25T17:23:48.424952Z","shell.execute_reply.started":"2025-04-25T17:23:18.176179Z","shell.execute_reply":"2025-04-25T17:23:48.424105Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:24:31.345894Z","iopub.execute_input":"2025-04-25T17:24:31.346258Z","iopub.status.idle":"2025-04-25T17:24:31.352874Z","shell.execute_reply.started":"2025-04-25T17:24:31.346225Z","shell.execute_reply":"2025-04-25T17:24:31.351931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def groundingdino(image_path):\n    for img in image_path:\n        image, image_tensor = load_image(img)\n        image_id  = extract_id_from_filename(img)\n        row = df[df[\"Id\"] == image_id].iloc[0]\n        caption = product_name = row[\"product_name\"] # it will be our prompt\n        device = next(model.parameters()).device\n        image_tensor = image_tensor.to(device=next(model.parameters()).device, dtype=torch.float32)\n        \n        boxes, logits, phrases = predict(\n            model=model,\n            image=image_tensor,\n            caption=caption,\n            box_threshold=0.3,\n            text_threshold=0.25,\n            device=str(device)\n        )\n\n        if logits is None or len(logits) == 0:\n            print(f\"No object detected in image: {img}\")\n            continue\n            \n        best_index = int(logits.argmax())\n        best_box = boxes[best_index].unsqueeze(0)\n        \n        annotated_frame = annotate(\n            image_source=image, \n            boxes=best_box, \n            phrases=[\"\"],\n            #logits=[logits[best_index]]\n        )\n        print('logits', logits)\n        print('boxes',best_box)\n        \n        plt.imshow(annotated_frame)\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:35:44.794591Z","iopub.execute_input":"2025-04-25T17:35:44.794929Z","iopub.status.idle":"2025-04-25T17:35:44.801545Z","shell.execute_reply.started":"2025-04-25T17:35:44.794904Z","shell.execute_reply":"2025-04-25T17:35:44.800768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'Küçük_El_Feneri_4_Adet' in name][:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:35:46.484376Z","iopub.execute_input":"2025-04-25T17:35:46.484697Z","iopub.status.idle":"2025-04-25T17:35:46.492141Z","shell.execute_reply.started":"2025-04-25T17:35:46.484668Z","shell.execute_reply":"2025-04-25T17:35:46.491259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"groundingdino(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:35:47.742389Z","iopub.execute_input":"2025-04-25T17:35:47.742717Z","iopub.status.idle":"2025-04-25T17:35:48.165627Z","shell.execute_reply.started":"2025-04-25T17:35:47.742686Z","shell.execute_reply":"2025-04-25T17:35:48.164509Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[df['product_name']=='Küçük_El_Feneri_4_Adet', ['product_name']] = 'plastic flashlights'\nimage_path = [name for name in full_photo_paths if 'Küçük_El_Feneri_4_Adet' in name][:10]\ngroundingdino(image_path) #im checking if i rename the prompt english","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T14:24:38.766181Z","iopub.execute_input":"2025-04-25T14:24:38.766594Z","iopub.status.idle":"2025-04-25T14:27:16.607402Z","shell.execute_reply.started":"2025-04-25T14:24:38.766557Z","shell.execute_reply":"2025-04-25T14:27:16.606313Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#as you see when i change the prompt it detected better.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T14:44:03.874531Z","iopub.execute_input":"2025-04-25T14:44:03.87512Z","iopub.status.idle":"2025-04-25T14:44:03.879985Z","shell.execute_reply.started":"2025-04-25T14:44:03.875085Z","shell.execute_reply":"2025-04-25T14:44:03.878855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'Profesyonel_El_Feneri_Usb_Şarjlı_6_Modlu_Mor_Işıklı_Çakarlı_Mıknatıslı_Özel_Kutulu' in name][:10]\ngroundingdino(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:01:50.648073Z","iopub.execute_input":"2025-04-25T15:01:50.648425Z","iopub.status.idle":"2025-04-25T15:02:22.195072Z","shell.execute_reply.started":"2025-04-25T15:01:50.648396Z","shell.execute_reply":"2025-04-25T15:02:22.193265Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11114792,"sourceType":"datasetVersion","datasetId":6930097},{"sourceId":11114969,"sourceType":"datasetVersion","datasetId":6930226},{"sourceId":11617060,"sourceType":"datasetVersion","datasetId":7287544},{"sourceId":11699834,"sourceType":"datasetVersion","datasetId":7343678},{"sourceId":11721147,"sourceType":"datasetVersion","datasetId":7357954},{"sourceId":355510,"sourceType":"modelInstanceVersion","modelInstanceId":296515,"modelId":317123},{"sourceId":357095,"sourceType":"modelInstanceVersion","modelInstanceId":297637,"modelId":318241}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"As you will see below, the dataset is not very consistent, and since there is no data from a device like a Ridar device that measures distance, I am conducting this work using images of over 25,000 products shared in the review sections of selected products on an e-commerce site, with the goal of answering the question: Can the dimensions of products be estimated solely by taking pictures from different angles?\n\n**My aim is not to achieve a successful result, but to answer the question of how accurately the dimensions can be predicted.**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch import nn, optim\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom PIL import Image\nimport random\nimport warnings\nimport cv2\nimport re\nimport matplotlib.pyplot as plt\nwarnings.simplefilter(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:06:39.002682Z","iopub.execute_input":"2025-05-09T18:06:39.002956Z","iopub.status.idle":"2025-05-09T18:06:39.287946Z","shell.execute_reply.started":"2025-05-09T18:06:39.002936Z","shell.execute_reply":"2025-05-09T18:06:39.287306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_folder_paths = '/kaggle/input/dimension-photos'\nimages_name=os.listdir(images_folder_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.018047Z","iopub.execute_input":"2025-05-07T16:07:08.018376Z","iopub.status.idle":"2025-05-07T16:07:08.485729Z","shell.execute_reply.started":"2025-05-07T16:07:08.018355Z","shell.execute_reply":"2025-05-07T16:07:08.485007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(images_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.486472Z","iopub.execute_input":"2025-05-07T16:07:08.486728Z","iopub.status.idle":"2025-05-07T16:07:08.492752Z","shell.execute_reply.started":"2025-05-07T16:07:08.486707Z","shell.execute_reply":"2025-05-07T16:07:08.491774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_name[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.493757Z","iopub.execute_input":"2025-05-07T16:07:08.493993Z","iopub.status.idle":"2025-05-07T16:07:08.51076Z","shell.execute_reply.started":"2025-05-07T16:07:08.493961Z","shell.execute_reply":"2025-05-07T16:07:08.509973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[name for name in images_name if 'English_Home' in name] # On average there are 60 photos of each product","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.511517Z","iopub.execute_input":"2025-05-07T16:07:08.511838Z","iopub.status.idle":"2025-05-07T16:07:08.53321Z","shell.execute_reply.started":"2025-05-07T16:07:08.511795Z","shell.execute_reply":"2025-05-07T16:07:08.532431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths = []\nfor image_name in images_name:\n    full_path = os.path.join(images_folder_paths,image_name)\n    full_photo_paths.append(full_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.535369Z","iopub.execute_input":"2025-05-07T16:07:08.535566Z","iopub.status.idle":"2025-05-07T16:07:08.582234Z","shell.execute_reply.started":"2025-05-07T16:07:08.535549Z","shell.execute_reply":"2025-05-07T16:07:08.581512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.583628Z","iopub.execute_input":"2025-05-07T16:07:08.58386Z","iopub.status.idle":"2025-05-07T16:07:08.599351Z","shell.execute_reply.started":"2025-05-07T16:07:08.583828Z","shell.execute_reply":"2025-05-07T16:07:08.598508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Image.open(full_photo_paths[100])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.600186Z","iopub.execute_input":"2025-05-07T16:07:08.600409Z","iopub.status.idle":"2025-05-07T16:07:08.676255Z","shell.execute_reply.started":"2025-05-07T16:07:08.600391Z","shell.execute_reply":"2025-05-07T16:07:08.67538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv = pd.read_csv('/kaggle/input/dimension-csv/finaldata.csv')\ndf_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.677028Z","iopub.execute_input":"2025-05-07T16:07:08.677245Z","iopub.status.idle":"2025-05-07T16:07:08.846963Z","shell.execute_reply.started":"2025-05-07T16:07:08.677225Z","shell.execute_reply":"2025-05-07T16:07:08.846079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv['product_name'] = df_csv['product_name'].str.replace('-', '_')\ndf_csv['product_name'] = df_csv['product_name'].str.replace(',', '')\ndf_csv['product_name'] = df_csv['product_name'].str.replace('(','')\ndf_csv['product_name'] = df_csv['product_name'].str.replace(')','')\ndf_csv['product_name'] = df_csv['product_name'].str.replace('™','')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.847776Z","iopub.execute_input":"2025-05-07T16:07:08.847994Z","iopub.status.idle":"2025-05-07T16:07:08.896095Z","shell.execute_reply.started":"2025-05-07T16:07:08.847974Z","shell.execute_reply":"2025-05-07T16:07:08.895369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.89699Z","iopub.execute_input":"2025-05-07T16:07:08.897293Z","iopub.status.idle":"2025-05-07T16:07:08.934049Z","shell.execute_reply.started":"2025-05-07T16:07:08.897256Z","shell.execute_reply":"2025-05-07T16:07:08.933327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_name[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.934865Z","iopub.execute_input":"2025-05-07T16:07:08.935191Z","iopub.status.idle":"2025-05-07T16:07:08.940189Z","shell.execute_reply.started":"2025-05-07T16:07:08.935145Z","shell.execute_reply":"2025-05-07T16:07:08.939344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_photo_paths[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.941007Z","iopub.execute_input":"2025-05-07T16:07:08.9413Z","iopub.status.idle":"2025-05-07T16:07:08.956522Z","shell.execute_reply.started":"2025-05-07T16:07:08.94127Z","shell.execute_reply":"2025-05-07T16:07:08.955875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_csv.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.957312Z","iopub.execute_input":"2025-05-07T16:07:08.957621Z","iopub.status.idle":"2025-05-07T16:07:08.979052Z","shell.execute_reply.started":"2025-05-07T16:07:08.957577Z","shell.execute_reply":"2025-05-07T16:07:08.978171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_only_name = []\nfor i in images_name:\n    images_only_name.append(i.rsplit('.', 1)[0])\n\nimages_only_name[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:08.979775Z","iopub.execute_input":"2025-05-07T16:07:08.979962Z","iopub.status.idle":"2025-05-07T16:07:09.009149Z","shell.execute_reply.started":"2025-05-07T16:07:08.979945Z","shell.execute_reply":"2025-05-07T16:07:09.008459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = ['Id','product_name','height','width','image_path']\ndf = pd.DataFrame(columns=keys)\n\nfor inx, i  in enumerate(df_csv['product_name']):\n    ID = df_csv['Id'].iloc[inx]\n    result = f\"{i}_{ID}\"\n    if result in images_only_name:\n        img_index = images_only_name.index(result)\n        df.loc[len(df)]=[df_csv['Id'].iloc[inx],df_csv['product_name'].iloc[inx], df_csv['height'].iloc[inx],df_csv['width'].iloc[inx],full_photo_paths[img_index]]\n\n    else:\n        print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:09.009937Z","iopub.execute_input":"2025-05-07T16:07:09.010165Z","iopub.status.idle":"2025-05-07T16:07:52.977767Z","shell.execute_reply.started":"2025-05-07T16:07:09.010145Z","shell.execute_reply":"2025-05-07T16:07:52.977012Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:52.978393Z","iopub.execute_input":"2025-05-07T16:07:52.978647Z","iopub.status.idle":"2025-05-07T16:07:52.988818Z","shell.execute_reply.started":"2025-05-07T16:07:52.978617Z","shell.execute_reply":"2025-05-07T16:07:52.987839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"product_name_list = []\nfor img_path in df['image_path']:\n    result1 = img_path.split('/')[4]\n    result2 = result1.rsplit('_', 1)[0]\n    product_name_list.append(result2)\n\nunique_listt = list(set(product_name_list))\nprint(len(unique_listt)) #i have 511 different product.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:52.989652Z","iopub.execute_input":"2025-05-07T16:07:52.989932Z","iopub.status.idle":"2025-05-07T16:07:53.033287Z","shell.execute_reply.started":"2025-05-07T16:07:52.989898Z","shell.execute_reply":"2025-05-07T16:07:53.032551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_listt[:5] #some examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.034145Z","iopub.execute_input":"2025-05-07T16:07:53.034501Z","iopub.status.idle":"2025-05-07T16:07:53.041526Z","shell.execute_reply.started":"2025-05-07T16:07:53.034468Z","shell.execute_reply":"2025-05-07T16:07:53.040792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_some_photos(product_name = '.', already_list = []):\n    images = list()\n    vertical_images=[]\n    counter = 0\n    gal_list = [name for name in full_photo_paths if product_name in name]\n    if len(already_list)>0:\n        gal_list = already_list\n        \n    for path in gal_list:\n        image = Image.open(path)\n        images.append(image)            \n        \n    fig, axes = plt.subplots(len(images)//5, len(images) //(len(images)//5), figsize=(15, len(images)//1.5))\n    for ax, img in zip(axes.ravel(), images):\n        ax.imshow(img)\n        ax.axis(\"off\")\n    plt.suptitle(f\"{str(len(images))} samples of {product_name} of the dataset\", fontsize=16, y=0.9)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.045976Z","iopub.execute_input":"2025-05-07T16:07:53.046198Z","iopub.status.idle":"2025-05-07T16:07:53.057324Z","shell.execute_reply.started":"2025-05-07T16:07:53.046178Z","shell.execute_reply":"2025-05-07T16:07:53.056617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show_some_photos('Rota_Hediyelik')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.059509Z","iopub.execute_input":"2025-05-07T16:07:53.059754Z","iopub.status.idle":"2025-05-07T16:07:53.073987Z","shell.execute_reply.started":"2025-05-07T16:07:53.059734Z","shell.execute_reply":"2025-05-07T16:07:53.07319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show_some_photos('B101_LED_Işık_Micro_USB_Type_C_Girişli_10.000_mAh_Taşınabilir_Şarj_Cihazı_Powerbank_Gri')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.074714Z","iopub.execute_input":"2025-05-07T16:07:53.074945Z","iopub.status.idle":"2025-05-07T16:07:53.091474Z","shell.execute_reply.started":"2025-05-07T16:07:53.074928Z","shell.execute_reply":"2025-05-07T16:07:53.090738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ****As you can see some pictures are irrelevant\n### so I am going to use VGG16 to determine these","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"## How does Keras handle resizing?\nKeras' image.load_img(img_path, target_size=(224, 224)) uses nearest neighbor interpolation (or another interpolation method) to directly stretch or shrink the image to the given size. This means:\n\nIf the image is larger, it will shrink.\n\nIf the image is smaller, it will expand.\n\nThe aspect ratio will not be preserved (it will distort the image instead of adding black bars).\n\n**So at first I need to convert the images to 224x224 so I'm going to use padding because I want to keep the ratio of photos**","metadata":{}},{"cell_type":"code","source":"#from collections import Counter\n#image_sizes = []\n#\n## Tüm görüntülerin boyutlarını topla\n#for path in full_photo_paths:\n#    img = Image.open(path)\n#    image_sizes.append(img.size) # (width, height)\n#\n#Counter(image_sizes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.09232Z","iopub.execute_input":"2025-05-07T16:07:53.092568Z","iopub.status.idle":"2025-05-07T16:07:53.10657Z","shell.execute_reply.started":"2025-05-07T16:07:53.092535Z","shell.execute_reply":"2025-05-07T16:07:53.105923Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Image.open(full_photo_paths[50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.107424Z","iopub.execute_input":"2025-05-07T16:07:53.107669Z","iopub.status.idle":"2025-05-07T16:07:53.12252Z","shell.execute_reply.started":"2025-05-07T16:07:53.107649Z","shell.execute_reply":"2025-05-07T16:07:53.121827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#counter = 0\n#for i in range(len(full_photo_paths)):\n#    img = Image.open(full_photo_paths[i])\n#    if img.size == (140, 311):\n#        Image.open(full_photo_paths[i])\n#        # I cant use img.show() beacuse of Kaggle\n#        plt.imshow(img)\n#        plt.axis(\"off\")\n#        plt.show()\n#        print(f\"index: {i}\")\n#        counter+=1\n#        if counter %5==0:\n#            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.123302Z","iopub.execute_input":"2025-05-07T16:07:53.123516Z","iopub.status.idle":"2025-05-07T16:07:53.139186Z","shell.execute_reply.started":"2025-05-07T16:07:53.123496Z","shell.execute_reply":"2025-05-07T16:07:53.138264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If all crops are at very different ratios, some images may have large black bands. The network may try to learn whether black (“0,0,0”) is the background or something important.","metadata":{}},{"cell_type":"code","source":"import cv2\ndef resize_with_padding(image_paths=None, target_size=(224, 224), image=None):\n    padded_images = []\n    if image_paths is not None:\n        \n        for img_path in image_paths:\n            # Load the image\n            image = cv2.imread(img_path)\n            if image is None:\n                raise ValueError(f\"Image at {img_path} could not be loaded.\")\n            \n            old_size = image.shape[:2]  # (height, width)\n            ratio = min(target_size[0] / old_size[0], target_size[1] / old_size[1])\n    \n            new_size = (int(old_size[1] * ratio), int(old_size[0] * ratio))  # (width, height)\n            resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n    \n            mean_color = list(map(int, resized_image.mean(axis=(0,1))))\n            delta_w, delta_h = target_size[1]-new_size[0], target_size[0]-new_size[1]\n            top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n            left, right = delta_w // 2, delta_w - (delta_w // 2)\n    \n            padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=mean_color)\n    \n            padded_images.append({'image': padded_image, 'path': img_path})\n        \n        return padded_images\n\n    if image is not None:\n        if not isinstance(image, np.ndarray):\n            image = np.array(image)\n\n        if image.ndim == 2:  # Eğer siyah-beyaz ise, 3 kanal yap\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n        old_size = image.shape[:2]  # (height, width)\n        ratio = min(target_size[0] / old_size[0], target_size[1] / old_size[1])\n        new_size = (int(old_size[1] * ratio), int(old_size[0] * ratio))  # (width, height)\n        resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n        mean_color = list(map(int, resized_image.mean(axis=(0,1))))\n        delta_w, delta_h = target_size[1]-new_size[0], target_size[0]-new_size[1]\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n        padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=mean_color)\n        return padded_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.14006Z","iopub.execute_input":"2025-05-07T16:07:53.140296Z","iopub.status.idle":"2025-05-07T16:07:53.706705Z","shell.execute_reply.started":"2025-05-07T16:07:53.140277Z","shell.execute_reply":"2025-05-07T16:07:53.705682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(full_photo_paths[96])\nplt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:53.70755Z","iopub.execute_input":"2025-05-07T16:07:53.707841Z","iopub.status.idle":"2025-05-07T16:07:54.021311Z","shell.execute_reply.started":"2025-05-07T16:07:53.707817Z","shell.execute_reply":"2025-05-07T16:07:54.02045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" cv2.imread(full_photo_paths[96]).shape[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:54.022221Z","iopub.execute_input":"2025-05-07T16:07:54.022487Z","iopub.status.idle":"2025-05-07T16:07:54.055043Z","shell.execute_reply.started":"2025-05-07T16:07:54.022464Z","shell.execute_reply":"2025-05-07T16:07:54.054349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg = Image.open(full_photo_paths[96])\n#img = np.array(img)\npadded_img = resize_with_padding(image=img, target_size=(224, 224))\n\nplt.imshow(cv2.cvtColor(padded_img, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:54.055866Z","iopub.execute_input":"2025-05-07T16:07:54.056107Z","iopub.status.idle":"2025-05-07T16:07:54.166231Z","shell.execute_reply.started":"2025-05-07T16:07:54.056081Z","shell.execute_reply":"2025-05-07T16:07:54.165397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"padded_images = resize_with_padding(image_paths = [full_photo_paths[96]], target_size=(224, 224))\n\n# Display the resized images using matplotlib\nfor i, img in enumerate(padded_images):\n    plt.subplot(1, len(padded_images), i + 1)\n    img_rgb = cv2.cvtColor(img['image'], cv2.COLOR_BGR2RGB)\n    plt.imshow(img_rgb)\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:54.167215Z","iopub.execute_input":"2025-05-07T16:07:54.167547Z","iopub.status.idle":"2025-05-07T16:07:54.263891Z","shell.execute_reply.started":"2025-05-07T16:07:54.167513Z","shell.execute_reply":"2025-05-07T16:07:54.262974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Detection","metadata":{}},{"cell_type":"markdown","source":"## Visual Outlier Detection with VGG16","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\ndef extract_features(img_path, model, device='cpu'):\n    img = Image.open(img_path).convert('RGB')\n\n    # Define preprocessing pipeline\n    preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n    ])\n    img_tensor = preprocess(img)\n    img_tensor = img_tensor.unsqueeze(0) # to add batch \n\n    img_tensor = img_tensor.to(device)\n    \n    model.eval() #extract features\n    with torch.no_grad(): #avoid waste of memory and something like that\n        features = model(img_tensor)\n    features = features.cpu().numpy().flatten()\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:54.264845Z","iopub.execute_input":"2025-05-07T16:07:54.265212Z","iopub.status.idle":"2025-05-07T16:07:54.27137Z","shell.execute_reply.started":"2025-05-07T16:07:54.265165Z","shell.execute_reply":"2025-05-07T16:07:54.270408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom sklearn.ensemble import IsolationForest\n\n#model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n#model.avgpool = nn.AdaptiveAvgPool2d(1)  # Global average pooling layer\n#model.classifier = nn.Identity()  # Remove the classification layer (optional)\n#\n#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n#model = model.to(device)\n#\n#product_name = 'Beyaz_Kupa_Bardak_Winnie_Arkadaşlık_Dostluk_Kardeşli'\n#image_files = [name for name in full_photo_paths if product_name in name]\n#\n## Extract features for each image\n#feature_vectors = np.array([extract_features(img, model) for img in image_files])\n#\n##PCA for dimensionality reduction:\n#pca = PCA(n_components=10)  \n#reduced_features = pca.fit_transform(feature_vectors)\n#\n## Cosine similarity hesaplama\n#similarity_matrix = cosine_similarity(reduced_features)\n#\n## Outlier tespiti için Isolation Forest kullanma\n#iso_forest = IsolationForest(contamination=0.1)  # Kontaminasyon oranını ayarlayabilirsin (0.1, %10'luk dilim)\n#outlier_predictions = iso_forest.fit_predict(reduced_features)\n#\n## Outlier fotoğrafları bulma\n#outlier_indices = np.where(outlier_predictions == -1)[0]  # -1 outlier anlamına gelir\n#outlier_images = [image_files[i] for i in outlier_indices]\n#\n#print(\"Outlier (kupa içermeyen) resimler:\", outlier_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:54.272242Z","iopub.execute_input":"2025-05-07T16:07:54.272484Z","iopub.status.idle":"2025-05-07T16:07:55.333347Z","shell.execute_reply.started":"2025-05-07T16:07:54.272463Z","shell.execute_reply":"2025-05-07T16:07:55.332652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show_some_photos(';',outlier_images) #but it isint work as i expected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.33398Z","iopub.execute_input":"2025-05-07T16:07:55.334369Z","iopub.status.idle":"2025-05-07T16:07:55.338015Z","shell.execute_reply.started":"2025-05-07T16:07:55.334346Z","shell.execute_reply":"2025-05-07T16:07:55.337099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If I crop the image to show only the object after detecting it in an image with yolo, the model cannot learn the auxiliary objects around that object (hand, ruler, etc. if there is one), but if I do not delete it, the model will be trained according to the objects around it instead of the object enclosed in a rectangle (the actual desired object), this time it will be an error. What should I do at this stage?","metadata":{}},{"cell_type":"markdown","source":"My decision is this: I need to crop because the backgrounds are so different and I have too many class (511) and every class has only 60 photos","metadata":{}},{"cell_type":"markdown","source":"## YOLO\n### YOLO (You Only Look Once) is a state-of-the-art object detection algorithm known for its speed and efficiency. It processes an entire image in a single forward pass of the network, predicting both the class and bounding box coordinates for multiple objects simultaneously.\n\n### YOLO models are pre-trained on datasets like COCO and can detect dozens of object classes in real-time. However, for detecting custom classes, the model must be fine-tuned or retrained on labeled data. While not as flexible as CLIP+SAM for zero-shot tasks, YOLO remains a top choice for real-time applications such as surveillance, autonomous driving, and robotics due to its high accuracy and low latency.\n\n","metadata":{}},{"cell_type":"code","source":"#from IPython.display import clear_output\n#\n#!pip install ultralytics\n#clear_output()\n#\n#from ultralytics import YOLO\n#model = YOLO(\"yolov8x.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.339091Z","iopub.execute_input":"2025-05-07T16:07:55.339416Z","iopub.status.idle":"2025-05-07T16:07:55.356145Z","shell.execute_reply.started":"2025-05-07T16:07:55.339383Z","shell.execute_reply":"2025-05-07T16:07:55.355507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_and_plot_from_ndarray_list(image_list, model):\n    records = []\n    for idx, image in enumerate(image_list):\n        if image is None or not isinstance(image['image'], np.ndarray):\n            print(f\"LList element {idx} is not a valid image, skipping.\")\n            continue\n            \n        temp_path = f\"temp_padded_{idx}.jpg\"\n        cv2.imwrite(temp_path, image['image'])\n\n        results = model(temp_path)[0]\n        img = cv2.cvtColor(image['image'], cv2.COLOR_BGR2RGB)\n\n        plt.figure(figsize=(6, 6))\n        plt.imshow(img)\n        plt.axis('off')\n        image_h, image_w = img.shape[:2]\n\n        if results.boxes:\n            best_box = max(results.boxes, key=lambda b: b.conf[0].item())\n\n            x1, y1, x2, y2 = best_box.xyxy[0].cpu().numpy()\n            cls = int(best_box.cls[0].item())\n            conf = float(best_box.conf[0].item())\n\n            box_width = x2 - x1\n            box_height = y2 - y1\n            box_area = box_width * box_height\n            box_center_x = x1 + box_width / 2\n            box_center_y = y1 + box_height / 2\n\n            bounding_box_ratio = box_width / box_height if box_height != 0 else 0\n            box_relative_width = box_width / image_w\n            box_relative_height = box_height / image_h\n            image_area = image_h * image_w\n\n            print('--------------------------------------------------------')\n            img_id = int(re.search(r'_(\\d+)\\.jpg', image['path']).group(1))\n            print(re.search(r'_(\\d+)\\.jpg', image['path']).group(1))\n            df.loc[df['Id']== img_id, ['bounding_box_width',\n                                       'bounding_box_height', \n                                       'bounding_box_ratio',\n                                       'box_relative_width', \n                                       'box_relative_height',\n                                       'confidence',\n                                       'image_area', \n                                       'box_area', \n                                       'box_center_x', 'box_center_y']] = [\n                                                                           box_width, \n                                                                           box_height, \n                                                                           bounding_box_ratio,   \n                                                                           box_relative_width, \n                                                                           box_relative_height, \n                                                                           conf, \n                                                                           image_area, \n                                                                           box_area, \n                                                                           box_center_x, \n                                                                           box_center_y]\n\n            \n            plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                              edgecolor='lime', facecolor='none', linewidth=2))\n            \n        else:\n            print(f\"⚠️ No box detected in image {idx}.\")\n\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.357011Z","iopub.execute_input":"2025-05-07T16:07:55.357304Z","iopub.status.idle":"2025-05-07T16:07:55.373807Z","shell.execute_reply.started":"2025-05-07T16:07:55.357274Z","shell.execute_reply":"2025-05-07T16:07:55.373001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = [name for name in full_photo_paths if 'Cep_Çanta_Aynas' in name][:5]\n\npadded_images = resize_with_padding(img_path, target_size=(224, 224))\n#detect_and_plot_from_ndarray_list(padded_images, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.374642Z","iopub.execute_input":"2025-05-07T16:07:55.374884Z","iopub.status.idle":"2025-05-07T16:07:55.457825Z","shell.execute_reply.started":"2025-05-07T16:07:55.374865Z","shell.execute_reply":"2025-05-07T16:07:55.456902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CLIP + SAM\n### CLIP (Contrastive Language–Image Pretraining) and SAM (Segment Anything Model) can be combined to enable a more flexible and language-guided form of object detection. CLIP maps both images and text into a shared embedding space, allowing users to specify objects using natural language (e.g., \"a person with a red hat\"). SAM, on the other hand, excels at segmenting objects at the pixel level with high precision.\n\n### When used together, CLIP identifies which parts of the image are semantically similar to the text prompt, while SAM precisely segments those regions. This combination is particularly powerful in zero-shot scenarios where no training on the specific object class is required. It’s ideal for interactive tools, content search, or dataset creation where labeled data is limited.\n\n","metadata":{}},{"cell_type":"markdown","source":"to use CLIP I need to rewrite the df['product_name'] in enligsh ","metadata":{}},{"cell_type":"code","source":"df.loc[df['product_name']=='glass_sphere', ['product_name']] = 'glass light emitting sphere'\ndf.loc[df['product_name']=='Potted_Cactu_Plush_Toy', ['product_name']] = 'Toy cactus'\ndf.loc[df['product_name']=='Tarak', ['product_name']] = 'black women comb'\n\n\ndf.loc[df['product_name']=='torch', ['product_name']] = 'flashlight in green box'\ndf.loc[df['product_name']=='glove2', ['product_name']] = 'white cloth'\ndf.loc[df['product_name']=='parfumm', ['product_name']] = 'perfume'\ndf.loc[df['product_name']=='Genel_Markalar', ['product_name']] = 'plastic  light emitting sphere'\ndf.loc[df['product_name']=='Orkide_Yetiştirme', ['product_name']] = 'black seed'\ndf.loc[df['product_name']=='BEYZANA', ['product_name']] = 'pencil bag'\ndf.loc[df['product_name']=='DEMPOWER', ['product_name']] = 'white small tool'\n\ndf.loc[df['product_name']=='Midilli', ['product_name']] = 'pink notebook'\ndf.loc[df['product_name']=='Tonny_Black', ['product_name']] = 'black leather wallet'\ndf.loc[df['product_name']=='LET_SCRUB', ['product_name']] = 'white cloth'\ndf.loc[df['product_name']=='woys', ['product_name']] = 'black handbag'\ndf.loc[df['product_name']=='Dekals', ['product_name']] = 'pink buckle'\ndf.loc[df['product_name']=='EMBHOME', ['product_name']] = 'steel ring iron'\ndf.loc[df['product_name']=='Paşabahçe', ['product_name']] = 'rectangular glass'\n#df.loc[df['product_name']=='Küçük_El_Feneri_4_Adet', ['product_name']] = 'plastic flashlights'\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.458669Z","iopub.execute_input":"2025-05-07T16:07:55.458908Z","iopub.status.idle":"2025-05-07T16:07:55.525532Z","shell.execute_reply.started":"2025-05-07T16:07:55.458888Z","shell.execute_reply":"2025-05-07T16:07:55.524675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = [name for name in full_photo_paths if 'Genel_Markalar' in name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.526473Z","iopub.execute_input":"2025-05-07T16:07:55.526754Z","iopub.status.idle":"2025-05-07T16:07:55.533731Z","shell.execute_reply.started":"2025-05-07T16:07:55.526732Z","shell.execute_reply":"2025-05-07T16:07:55.532644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['product_name'] == 'rectangular glass']","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.534664Z","iopub.execute_input":"2025-05-07T16:07:55.534971Z","iopub.status.idle":"2025-05-07T16:07:55.577107Z","shell.execute_reply.started":"2025-05-07T16:07:55.534941Z","shell.execute_reply":"2025-05-07T16:07:55.57631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install git+https://github.com/openai/CLIP.git\n#!pip install opencv-python matplotlib\n#!pip install git+https://github.com/facebookresearch/segment-anything.git\n#clear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.57802Z","iopub.execute_input":"2025-05-07T16:07:55.57829Z","iopub.status.idle":"2025-05-07T16:07:55.59717Z","shell.execute_reply.started":"2025-05-07T16:07:55.578253Z","shell.execute_reply":"2025-05-07T16:07:55.596452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"padded_images = resize_with_padding(img_path, target_size=(224, 224))\ntype(padded_images[0]['image'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.598064Z","iopub.execute_input":"2025-05-07T16:07:55.598334Z","iopub.status.idle":"2025-05-07T16:07:55.929854Z","shell.execute_reply.started":"2025-05-07T16:07:55.598312Z","shell.execute_reply":"2025-05-07T16:07:55.928868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_id_from_filename(filename):\n    # Get the last part after the last underscore and before the file extension\n    last_part = filename.split(\"_\")[-1].split(\".\")[0]\n\n    # If not a digit, try to clean it\n    if not last_part.isdigit():\n        last_part = last_part.replace(\"-checkpoint\", \"\")\n\n    return int(last_part)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.930746Z","iopub.execute_input":"2025-05-07T16:07:55.93098Z","iopub.status.idle":"2025-05-07T16:07:55.93513Z","shell.execute_reply.started":"2025-05-07T16:07:55.93096Z","shell.execute_reply":"2025-05-07T16:07:55.934301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from segment_anything import sam_model_registry, SamPredictor\n#import clip\nfrom tqdm import tqdm\n\n\nimages_path = \"/kaggle/input/dimension-photos\" \nsam_checkpoint = \"/kaggle/input/sam_model/pytorch/default/1/sam_vit_h_4b8939.pth\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# CLIP\n#clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# SAM\n#sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint).to(device)\n#predictor = SamPredictor(sam)\n\n\ndef run_clip_sam(image_path, prompt):\n    \n    image = Image.open(image_path).convert(\"RGB\")\n\n    padded_image = resize_with_padding([image_path], target_size=(224, 224))\n    image_np = np.array(padded_image[0]['image'])\n    image_tensor = clip_preprocess(image).unsqueeze(0).to(device)\n\n    # CLIP: prompt and the picture\n    with torch.no_grad():\n        text = clip.tokenize([prompt]).to(device)\n        image_features = clip_model.encode_image(image_tensor)\n        text_features = clip_model.encode_text(text)\n        similarity = torch.cosine_similarity(image_features, text_features).item()\n\n    # SAM segmentation\n    predictor.set_image(image_np)\n    masks, scores, _ = predictor.predict(box=None, multimask_output=True)\n\n    if masks is None or len(masks) == 0:\n        return None\n\n    best_mask = masks[np.argmax(scores)]\n    ys, xs = np.where(best_mask)\n    if len(xs) == 0 or len(ys) == 0:\n        return None\n    x_min, x_max, y_min, y_max = xs.min(), xs.max(), ys.min(), ys.max()\n\n    # Normalize (YOLO formatı)\n    h, w = image_np.shape[:2]\n    x_center = ((x_min + x_max) / 2) / w\n    y_center = ((y_min + y_max) / 2) / h\n    box_w = (x_max - x_min) / w\n    box_h = (y_max - y_min) / h\n\n    return [x_center, y_center, box_w, box_h]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:55.936031Z","iopub.execute_input":"2025-05-07T16:07:55.936393Z","iopub.status.idle":"2025-05-07T16:07:56.082567Z","shell.execute_reply.started":"2025-05-07T16:07:55.936339Z","shell.execute_reply":"2025-05-07T16:07:56.081857Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_box_on_image(image_path, box):\n    padded_image = resize_with_padding([image_path], target_size=(224, 224))\n    image = padded_image[0]['image']\n    \n    h, w = image.shape[:2]\n\n    x_center, y_center, box_w, box_h = box\n    x_min = int((x_center - box_w / 2) * w)\n    x_max = int((x_center + box_w / 2) * w)\n    y_min = int((y_center - box_h / 2) * h)\n    y_max = int((y_center + box_h / 2) * h)\n\n    # Kutuyu çiz\n    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n\n    plt.figure(figsize=(6, 6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:56.08335Z","iopub.execute_input":"2025-05-07T16:07:56.083651Z","iopub.status.idle":"2025-05-07T16:07:56.089441Z","shell.execute_reply.started":"2025-05-07T16:07:56.083607Z","shell.execute_reply":"2025-05-07T16:07:56.08874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:56.090313Z","iopub.execute_input":"2025-05-07T16:07:56.090544Z","iopub.status.idle":"2025-05-07T16:07:56.114552Z","shell.execute_reply.started":"2025-05-07T16:07:56.090523Z","shell.execute_reply":"2025-05-07T16:07:56.113666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['product_name'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:56.115563Z","iopub.execute_input":"2025-05-07T16:07:56.115877Z","iopub.status.idle":"2025-05-07T16:07:56.137376Z","shell.execute_reply.started":"2025-05-07T16:07:56.115855Z","shell.execute_reply":"2025-05-07T16:07:56.136565Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for i, image_file in enumerate(sorted(img_path[:10])):\n#    image_id = extract_id_from_filename(image_file)\n#    row = df[df[\"Id\"] == image_id].iloc[0]\n#    product_name = row[\"product_name\"]\n#    image_path_full = os.path.join(images_path, image_file)\n#    box = run_clip_sam(image_path_full, prompt=product_name)\n#    if box:\n#        print(f\"Image: {image_file} | Prompt: {product_name}\")\n#        draw_box_on_image(image_path_full, box)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:56.138215Z","iopub.execute_input":"2025-05-07T16:07:56.13841Z","iopub.status.idle":"2025-05-07T16:07:56.141975Z","shell.execute_reply.started":"2025-05-07T16:07:56.138392Z","shell.execute_reply":"2025-05-07T16:07:56.141082Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it put in a boxt all of the photos","metadata":{}},{"cell_type":"markdown","source":"## GroundingDINO","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(\"GroundingDINO\"):\n    !git clone https://github.com/IDEA-Research/GroundingDINO.git\n\n%cd GroundingDINO\n!pip install -e .\n!pip install -q groundingdino transformers torchvision matplotlib\n!pip install -q -e GroundingDINO\n!pip install -q git+https://github.com/openai/CLIP.git\nfrom IPython.display import clear_output\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:07:56.142869Z","iopub.execute_input":"2025-05-07T16:07:56.143065Z","iopub.status.idle":"2025-05-07T16:09:03.466568Z","shell.execute_reply.started":"2025-05-07T16:07:56.143047Z","shell.execute_reply":"2025-05-07T16:09:03.465371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport os\n\nmodel_url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/0.1.0/groundingdino_swint_ogc.pth\"\nmodel_path = \"/kaggle/input/groundingdino_model/pytorch/default/1/groundingdino_swint_ogc.pth\"\n\nif not os.path.exists(model_path):\n    print(\"📥 Model downloading with stream...\")\n    with requests.get(model_url, stream=True) as r:\n        r.raise_for_status()\n        with open(model_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n    print(\"✅ Model downloaded.\")\nelse:\n    print(\"✅ Model already available.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:03.468052Z","iopub.execute_input":"2025-05-07T16:09:03.468439Z","iopub.status.idle":"2025-05-07T16:09:03.615545Z","shell.execute_reply.started":"2025-05-07T16:09:03.468399Z","shell.execute_reply":"2025-05-07T16:09:03.61481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from groundingdino.util.inference import load_model, load_image, predict, annotate\n\n# Load the Model\nconfig_path = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\nmodel = load_model(config_path, model_path)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:03.616442Z","iopub.execute_input":"2025-05-07T16:09:03.616785Z","iopub.status.idle":"2025-05-07T16:09:32.947246Z","shell.execute_reply.started":"2025-05-07T16:09:03.616752Z","shell.execute_reply":"2025-05-07T16:09:32.946012Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def groundingdino(image_path):\n    for img in image_path:\n        image, image_tensor = load_image(img)\n        image_id  = extract_id_from_filename(img)\n        row = df[df[\"Id\"] == image_id].iloc[0]\n        caption = product_name = row[\"product_name\"] # it will be our prompt\n        device = next(model.parameters()).device\n        image_tensor = image_tensor.to(device=next(model.parameters()).device, dtype=torch.float32)\n        \n        boxes, logits, phrases = predict(\n            model=model,\n            image=image_tensor,\n            caption=caption,\n            box_threshold=0.3,\n            text_threshold=0.25,\n            device=str(device)\n        )\n\n        if logits is None or len(logits) == 0:\n            print(f\"No object detected in image: {img}\")\n            continue\n            \n        best_index = int(logits.argmax())\n        best_box = boxes[best_index].unsqueeze(0)\n        \n        x_center, y_center, width, height = best_box[0].tolist()\n        x_min = x_center - width / 2\n        y_min = y_center - height / 2\n        x_max = x_center + width / 2\n        y_max = y_center + height / 2\n\n        image_height, image_width = image.shape[:2]\n        \n        x_min = int(x_min * image_width)\n        y_min = int(y_min * image_height)\n        x_max = int(x_max * image_width)\n        y_max = int(y_max * image_height)\n        \n        print(f\"Cropped image size: {(y_max-y_min)}x{(x_max-x_min)}\")\n        \n        annotated_frame = annotate(\n            image_source=image, \n            boxes=best_box, \n            phrases=[\"\"],\n            logits=[logits[best_index]]\n        )\n        print('logits', logits)\n        print('boxes',best_box)\n        \n        plt.imshow(annotated_frame)\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:32.948351Z","iopub.execute_input":"2025-05-07T16:09:32.948684Z","iopub.status.idle":"2025-05-07T16:09:32.958645Z","shell.execute_reply.started":"2025-05-07T16:09:32.948656Z","shell.execute_reply":"2025-05-07T16:09:32.957694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n[name for name in full_photo_paths if 'Duvar_Saati_Tinatti_Gold_Saat_Aynalı_Saat_Roma_Rakamlı_Aynalı_Gold_Saat_Duvar_Saati' in name][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:32.959639Z","iopub.execute_input":"2025-05-07T16:09:32.959961Z","iopub.status.idle":"2025-05-07T16:09:32.986404Z","shell.execute_reply.started":"2025-05-07T16:09:32.959926Z","shell.execute_reply":"2025-05-07T16:09:32.985696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'Küçük_El_Feneri_4_Adet' in name][:5]\ngroundingdino(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:32.987385Z","iopub.execute_input":"2025-05-07T16:09:32.987749Z","iopub.status.idle":"2025-05-07T16:09:37.836205Z","shell.execute_reply.started":"2025-05-07T16:09:32.987687Z","shell.execute_reply":"2025-05-07T16:09:37.835331Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[df['product_name']=='Küçük_El_Feneri_4_Adet', ['product_name']] = 'plastic flashlights'\nimage_path = [name for name in full_photo_paths if 'Küçük_El_Feneri_4_Adet' in name][:5]\ngroundingdino(image_path) #im checking if i rename the prompt english","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:37.837082Z","iopub.execute_input":"2025-05-07T16:09:37.83733Z","iopub.status.idle":"2025-05-07T16:09:39.672625Z","shell.execute_reply.started":"2025-05-07T16:09:37.837308Z","shell.execute_reply":"2025-05-07T16:09:39.671633Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!rm -r /kaggle/working/GroundingDINO/cropped_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.673673Z","iopub.execute_input":"2025-05-07T16:09:39.673989Z","iopub.status.idle":"2025-05-07T16:09:39.677747Z","shell.execute_reply.started":"2025-05-07T16:09:39.673964Z","shell.execute_reply":"2025-05-07T16:09:39.676862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### I tried and it is the best object detection model for my data so im gonna creat a func for the model","metadata":{"execution":{"iopub.status.busy":"2025-04-27T13:42:49.111593Z","iopub.execute_input":"2025-04-27T13:42:49.112034Z","iopub.status.idle":"2025-04-27T13:42:49.116887Z","shell.execute_reply.started":"2025-04-27T13:42:49.112Z","shell.execute_reply":"2025-04-27T13:42:49.115661Z"}}},{"cell_type":"code","source":"df_box = pd.DataFrame()\ndf_box['image_id'] = ''\ndf_box['image_width'] = 0\ndf_box['image_height'] = 0\ndf_box['bounding_box_width'] = 0.0\ndf_box['bounding_box_height'] = 0.0\ndf_box['box_relative_width'] = 0.0  # bounding_box_width / image_width\ndf_box['box_relative_height'] = 0.0  # bounding_box_height / image_height\ndf_box['confidence'] = 0.0  # Confidence score\ndf_box['box_area'] = 0.0\ndf_box['bbox_aspect_ratio'] = 0  # bounding_box_width / bounding_box_height\ndf_box['bbox_diag'] = 0  # sqrt(bw²+bh²)\ndf_box['log_box_area'] = 0  # log_box_area\ndf_box['norm_confidence'] = 0  # Normalized confidence\n\ndf_box.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.678716Z","iopub.execute_input":"2025-05-07T16:09:39.679026Z","iopub.status.idle":"2025-05-07T16:09:39.705156Z","shell.execute_reply.started":"2025-05-07T16:09:39.678992Z","shell.execute_reply":"2025-05-07T16:09:39.703847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# These code make the prompts better: I am translating the product_name and adding some futures ","metadata":{}},{"cell_type":"code","source":"df_with_new_prompt = pd.read_csv('/kaggle/input/df-with-new-prompt2/df_with_new_prompt (2).csv')\ndf_with_new_prompt.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:07:55.141662Z","iopub.execute_input":"2025-05-09T18:07:55.14198Z","iopub.status.idle":"2025-05-09T18:07:55.270884Z","shell.execute_reply.started":"2025-05-09T18:07:55.141955Z","shell.execute_reply":"2025-05-09T18:07:55.270061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_new_prompt[df_with_new_prompt['product_name']=='sil'] #im gonna delete them","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.834123Z","iopub.execute_input":"2025-05-07T16:09:39.834485Z","iopub.status.idle":"2025-05-07T16:09:39.84945Z","shell.execute_reply.started":"2025-05-07T16:09:39.834439Z","shell.execute_reply":"2025-05-07T16:09:39.848724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#different_prompt = df_with_new_prompt['product_name'].unique()\n#for col in different_prompt:\n#    if  '_' not in col:\n#        continue\n#    \n#    row = df_with_new_prompt[df_with_new_prompt[\"product_name\"] == col].iloc[0]\n#    print(col.replace('_',\" \"))\n#    padded = resize_with_padding(image_paths=[row['image_path']], target_size=(224, 224))\n#    plt.imshow(cv2.cvtColor(padded[0]['image'], cv2.COLOR_BGR2RGB))\n#    plt.axis('off')\n#    plt.show()\n#    user_input = input(\"New promopt: \") \n#    df_with_new_prompt.loc[df_with_new_prompt['product_name']== col, ['product_name']] = user_input\n#    #df.to_csv(\"/kaggle/working/df_with_new_prompt.csv\", index=False)\n#    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.85067Z","iopub.execute_input":"2025-05-07T16:09:39.851019Z","iopub.status.idle":"2025-05-07T16:09:39.867685Z","shell.execute_reply.started":"2025-05-07T16:09:39.850986Z","shell.execute_reply":"2025-05-07T16:09:39.866953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'glass_sphere' in name][:5]\nimage_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.868494Z","iopub.execute_input":"2025-05-07T16:09:39.868954Z","iopub.status.idle":"2025-05-07T16:09:39.886414Z","shell.execute_reply.started":"2025-05-07T16:09:39.868922Z","shell.execute_reply":"2025-05-07T16:09:39.88558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = df[df[\"product_name\"] == 'glass light emitting sphere'].iloc[0]\nrow['image_path']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.887289Z","iopub.execute_input":"2025-05-07T16:09:39.88756Z","iopub.status.idle":"2025-05-07T16:09:39.90449Z","shell.execute_reply.started":"2025-05-07T16:09:39.887525Z","shell.execute_reply":"2025-05-07T16:09:39.903752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'glass_sphere' in name]\ndef check_for_wide_images(image_path):\n    for img in image_path:\n        img_arr = cv2.imread(img)\n        if img_arr is None:\n            print(f\"❌ Could not read: {img}\")\n            continue\n        h, w = img_arr.shape[:2]\n        if w > h:\n            print(f\"✅ Wide image found: {img}\")\n            plt.imshow(img_arr)\n            plt.axis('off')\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.911892Z","iopub.execute_input":"2025-05-07T16:09:39.912139Z","iopub.status.idle":"2025-05-07T16:09:39.922197Z","shell.execute_reply.started":"2025-05-07T16:09:39.912118Z","shell.execute_reply":"2025-05-07T16:09:39.92135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = '/kaggle/input/dimension-photos/glass_sphere_54.jpg'\nimg_arr = cv2.imread(img)\nimg_arr.shape[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.924431Z","iopub.execute_input":"2025-05-07T16:09:39.924696Z","iopub.status.idle":"2025-05-07T16:09:39.95982Z","shell.execute_reply.started":"2025-05-07T16:09:39.924675Z","shell.execute_reply":"2025-05-07T16:09:39.959001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_for_wide_images(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:39.960653Z","iopub.execute_input":"2025-05-07T16:09:39.96094Z","iopub.status.idle":"2025-05-07T16:09:40.851521Z","shell.execute_reply.started":"2025-05-07T16:09:39.960911Z","shell.execute_reply":"2025-05-07T16:09:40.850457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### If the user uploaded the product image horizontally instead of the way it should be...","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef determine_long_or_wide(crop_dict):\n    list_of_size = []\n    for img in crop_dict:\n        img_arr = img['cropped_image']\n        h,w = img_arr.shape[:2]\n        \n        image_id = img['image_id']\n        \n        if h >= w:\n            list_of_size.append({'image_array':img_arr, 'image_id':image_id,'label':'Long'})\n        else:\n            list_of_size.append({'image_array':img_arr,'image_id':image_id, 'label':'Wide'})\n    \n    labels = [item['label'] for item in list_of_size]\n    counter = Counter(labels)\n    #print(\"Dominant:\", \"Long\" if counter['Long'] >= counter['Wide'] else \"Wide\")\n    #print(\"Counter:\", counter)\n    \n    if counter['Long'] >= counter['Wide']:\n        not_dominant = 'Wide'\n    else:\n        not_dominant = 'Long'\n\n    return not_dominant\n        \n    #for rotate in list_of_size:\n    #    if rotate['label']== not_dominant:\n    #        rotated = np.rot90(rotate['image_array'])\n    #        plt.imshow(cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB))  # Convert for correct color\n    #        plt.axis('off')\n    #        plt.show()\n    #        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:40.852546Z","iopub.execute_input":"2025-05-07T16:09:40.852948Z","iopub.status.idle":"2025-05-07T16:09:40.859031Z","shell.execute_reply.started":"2025-05-07T16:09:40.852908Z","shell.execute_reply":"2025-05-07T16:09:40.85796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def groundingdino_last_version(image_path):\n    output_crop_dir=\"cropped_images\"\n    os.makedirs(output_crop_dir, exist_ok=True)\n    crop_images_list = []\n    \n    for img in tqdm(image_path, desc=\"Processing Images\"):\n        image_id  = extract_id_from_filename(img)\n        \n        if image_id in df_box['image_id'].values:\n            continue\n\n        \n        image, image_tensor = load_image(img)\n        matched_rows = df_with_new_prompt[df_with_new_prompt[\"Id\"] == image_id]\n        if not matched_rows.empty:\n            row = matched_rows.iloc[0]\n        else:\n            continue  # skip this iteration if no match\n\n        caption = product_name = row[\"product_name\"] # it will be our prompt\n        device = next(model.parameters()).device\n        image_tensor = image_tensor.to(device=device, dtype=torch.float32)\n        \n        #model pridect \n        boxes, logits, phrases = predict(\n            model=model,\n            image=image_tensor,\n            caption=caption,\n            box_threshold=0.3,\n            text_threshold=0.25,\n            device=str(device)\n        )\n\n        if logits is None or len(logits) == 0:\n            print(f\"No object detected in image: {img}\")\n            continue\n            \n        best_index = int(logits.argmax())\n        best_box = boxes[best_index].unsqueeze(0)\n        \n        #detected the area\n        x_center, y_center, width, height = best_box[0].tolist()\n        x_min = x_center - width / 2\n        y_min = y_center - height / 2\n        x_max = x_center + width / 2\n        y_max = y_center + height / 2\n        \n        #print(f\"x_min={x_min:.3f}, y_min={y_min:.3f}, x_max={x_max:.3f}, y_max={y_max:.3f}\")\n\n        image_height, image_width = image.shape[:2]\n        \n\n        #crop\n        x_min = int(x_min * image_width)\n        y_min = int(y_min * image_height)\n        x_max = int(x_max * image_width)\n        y_max = int(y_max * image_height)\n\n        crop = image[y_min:y_max, x_min:x_max]\n\n        print(f\"Cropped image size: {(y_max-y_min)}x{(x_max-x_min)}\")\n        product_name = product_name.strip().replace(' ', '_')\n        \n        crop_images_list.append({'image_id':image_id,'cropped_image': crop, 'pre_height':image_height, 'pre_width':image_width, \n                                 'logits':logits,'product_name':product_name}) \n        \n        \n    not_dominant = determine_long_or_wide(crop_images_list)\n\n    for img in crop_images_list:\n        \n        image_id = img['image_id']\n        image_width = img['pre_width']\n        image_height = img['pre_height']\n        product_name = img['product_name']\n        \n        cropped = img['cropped_image']\n        \n        h,w = cropped.shape[:2]    \n        \n        if h >= w:\n            label = 'Long'\n        else:\n            label = 'Wide'\n        \n        if label == not_dominant:\n            cropped = np.rot90(cropped)\n\n        \n        \n        box_area = w * h\n        bbox_aspect_ratio = w / h if h != 0 else 0\n        bbox_diag = np.sqrt(w**2 + h**2)\n        \n        log_box_area = np.log(box_area) if box_area > 0 else 0\n        logits = img['logits']\n        confidence = logits.max().item()\n        norm_confidence = confidence / logits.sum().item() if logits.sum().item() > 0 else 0\n        \n        if logits.shape[0] > 0:  # detection found\n            \n            df_box.loc[len(df_box)] = [\n                image_id,\n                image_width,\n                image_height,\n                w,\n                h,\n                w / image_width,\n                h / image_height,\n                confidence,\n                box_area,\n                bbox_aspect_ratio,\n                bbox_diag,\n                log_box_area,\n                norm_confidence\n            ]\n        else:  # no detection\n            df_box.loc[len(df_box)] = [\n                image_id,\n                image_width,\n                image_height,\n                0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n            ]\n\n\n        #reshape\n        padded = resize_with_padding(image=cropped, target_size=(224, 224))\n        \n        #record the path\n        new_path = f\"{product_name}_{image_id}.jpg\"\n        \n        out_path = os.path.join(output_crop_dir, os.path.basename(new_path))\n        cv2.imwrite(out_path, padded)\n\n        \n        #plt.imshow(cv2.cvtColor(padded, cv2.COLOR_BGR2RGB))\n        #plt.axis('off')\n        #plt.show()\n\n    df_box.to_csv(\"/kaggle/working/df_box.csv\", index=False)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:40.860128Z","iopub.execute_input":"2025-05-07T16:09:40.860489Z","iopub.status.idle":"2025-05-07T16:09:40.877088Z","shell.execute_reply.started":"2025-05-07T16:09:40.860432Z","shell.execute_reply":"2025-05-07T16:09:40.876319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = [name for name in full_photo_paths if 'glass_sphere' in name][30:34]\ngroundingdino(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:40.878019Z","iopub.execute_input":"2025-05-07T16:09:40.878336Z","iopub.status.idle":"2025-05-07T16:09:42.51299Z","shell.execute_reply.started":"2025-05-07T16:09:40.878304Z","shell.execute_reply":"2025-05-07T16:09:42.51198Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paths = full_photo_paths\nunique_product_type = []\nfor path in paths:\n    filename = os.path.splitext(os.path.basename(path))[0]  # → 'glass_sphere_0'\n\n# Step 2: Remove the last underscore and number\n    product_type = '_'.join(filename.split('_')[:-1])  # → 'glass_sphere'\n    unique_product_type.append(product_type)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:42.513975Z","iopub.execute_input":"2025-05-07T16:09:42.514393Z","iopub.status.idle":"2025-05-07T16:09:42.617616Z","shell.execute_reply.started":"2025-05-07T16:09:42.514349Z","shell.execute_reply":"2025-05-07T16:09:42.616853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_product_type = list(set(unique_product_type))\nlen(unique_product_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:42.618493Z","iopub.execute_input":"2025-05-07T16:09:42.618784Z","iopub.status.idle":"2025-05-07T16:09:42.628577Z","shell.execute_reply.started":"2025-05-07T16:09:42.618759Z","shell.execute_reply":"2025-05-07T16:09:42.627703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"only_in_list1 = list(set(unique_product_type) - set(unique_listt))\nprint(\"Only in list1:\", only_in_list1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:42.629437Z","iopub.execute_input":"2025-05-07T16:09:42.629703Z","iopub.status.idle":"2025-05-07T16:09:42.645786Z","shell.execute_reply.started":"2025-05-07T16:09:42.62968Z","shell.execute_reply":"2025-05-07T16:09:42.644742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"remove_them = []\nfor nameff in unique_product_type:\n    image_path = [name for name in full_photo_paths if nameff in name]\n    if len(image_path) == 1:\n        remove_them.append(nameff)\n\nremove_them","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:42.64696Z","iopub.execute_input":"2025-05-07T16:09:42.647296Z","iopub.status.idle":"2025-05-07T16:09:44.171864Z","shell.execute_reply.started":"2025-05-07T16:09:42.647261Z","shell.execute_reply":"2025-05-07T16:09:44.170884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_list = [item for item in unique_product_type if item not in remove_them]\n\nlen(filtered_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:44.172969Z","iopub.execute_input":"2025-05-07T16:09:44.173336Z","iopub.status.idle":"2025-05-07T16:09:44.17886Z","shell.execute_reply.started":"2025-05-07T16:09:44.173283Z","shell.execute_reply":"2025-05-07T16:09:44.178111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for name_of_product in filtered_list:\n    #image_path = [name for name in full_photo_paths if name_of_product in name]\n    #groundingdino_last_version(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:44.179689Z","iopub.execute_input":"2025-05-07T16:09:44.179968Z","iopub.status.idle":"2025-05-07T16:09:44.193936Z","shell.execute_reply.started":"2025-05-07T16:09:44.179939Z","shell.execute_reply":"2025-05-07T16:09:44.193134Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(full_photo_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:44.194806Z","iopub.execute_input":"2025-05-07T16:09:44.195088Z","iopub.status.idle":"2025-05-07T16:09:44.210842Z","shell.execute_reply.started":"2025-05-07T16:09:44.195059Z","shell.execute_reply":"2025-05-07T16:09:44.209973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#folder_path  = '/kaggle/working/GroundingDINO/cropped_images'\n#image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n#print(len(image_files))\n#if image_files:\n#    image_path = os.path.join(folder_path, image_files[2])\n#    img = cv2.imread(image_path)\n#    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib\n#    plt.imshow(img)\n#    plt.title(image_files[2])\n#    plt.axis('off')\n#    plt.show()\n#else:\n#    print(\"No images found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:44.211576Z","iopub.execute_input":"2025-05-07T16:09:44.211849Z","iopub.status.idle":"2025-05-07T16:09:44.225452Z","shell.execute_reply.started":"2025-05-07T16:09:44.211822Z","shell.execute_reply":"2025-05-07T16:09:44.2247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_csv_path = '/kaggle/input/cropped-images-csv/df_box.csv'\ndf_cropped_csv = pd.read_csv(new_csv_path)\nlen(df_cropped_csv)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_path  = '/kaggle/input/cropped-images-file'\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\nlen(image_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:50:05.970211Z","iopub.execute_input":"2025-05-09T11:50:05.970524Z","iopub.status.idle":"2025-05-09T11:50:05.98664Z","shell.execute_reply.started":"2025-05-09T11:50:05.9705Z","shell.execute_reply":"2025-05-09T11:50:05.985576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### some cropped images","metadata":{}},{"cell_type":"code","source":"for i, filename in enumerate(image_files[:10]):\n    path = os.path.join(folder_path, filename)\n    img = cv2.imread(path)\n    plt.imshow(img)\n    plt.title(filename)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:44.608254Z","iopub.execute_input":"2025-05-07T16:09:44.608507Z","iopub.status.idle":"2025-05-07T16:09:47.139407Z","shell.execute_reply.started":"2025-05-07T16:09:44.608484Z","shell.execute_reply":"2025-05-07T16:09:47.138339Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idd = df_cropped_csv['image_id']\ncsv_ids_old = set(idd)\nmissing_ids = [i for i in idd if i not in id_list]\nlen(missing_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:55:01.268037Z","iopub.execute_input":"2025-05-07T16:55:01.26837Z","iopub.status.idle":"2025-05-07T16:55:07.988256Z","shell.execute_reply.started":"2025-05-07T16:55:01.26834Z","shell.execute_reply":"2025-05-07T16:55:07.987538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id_list = []\nfor i in image_files:\n    extract_id_from_filename(i)\n    id_list.append(extract_id_from_filename(i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:53:40.83813Z","iopub.execute_input":"2025-05-07T16:53:40.838469Z","iopub.status.idle":"2025-05-07T16:53:40.880638Z","shell.execute_reply.started":"2025-05-07T16:53:40.838445Z","shell.execute_reply":"2025-05-07T16:53:40.879677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"old_id = []\nfor idd in full_photo_paths:\n    old_id.append(extract_id_from_filename(idd))\n\ncsv_ids_old = set(old_id)\nmissing_ids = [i for i in old_id if i not in id_list]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:53:46.755117Z","iopub.execute_input":"2025-05-07T16:53:46.755468Z","iopub.status.idle":"2025-05-07T16:53:49.696548Z","shell.execute_reply.started":"2025-05-07T16:53:46.755438Z","shell.execute_reply":"2025-05-07T16:53:49.695854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Those without object detection","metadata":{}},{"cell_type":"code","source":"for i in missing_ids[10:20]:\n    matched_rows = df_with_new_prompt[df_with_new_prompt['Id'] == i]\n    \n    if not matched_rows.empty:\n        path = matched_rows['image_path'].values[0]\n        \n        img = cv2.imread(path)\n        if img is not None:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            plt.imshow(img)\n            plt.title(i)\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:09:50.280702Z","iopub.execute_input":"2025-05-07T16:09:50.280994Z","iopub.status.idle":"2025-05-07T16:09:52.440273Z","shell.execute_reply.started":"2025-05-07T16:09:50.28097Z","shell.execute_reply":"2025-05-07T16:09:52.439359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_cropped_csv[df_cropped_csv.duplicated()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:56:08.670306Z","iopub.execute_input":"2025-05-07T16:56:08.670654Z","iopub.status.idle":"2025-05-07T16:56:08.689208Z","shell.execute_reply.started":"2025-05-07T16:56:08.670586Z","shell.execute_reply":"2025-05-07T16:56:08.688161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv = df_cropped_csv.drop_duplicates()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T08:59:24.069523Z","iopub.execute_input":"2025-05-09T08:59:24.06986Z","iopub.status.idle":"2025-05-09T08:59:24.095926Z","shell.execute_reply.started":"2025-05-09T08:59:24.069833Z","shell.execute_reply":"2025-05-09T08:59:24.095254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv[df_cropped_csv['image_id']==0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T08:59:25.75843Z","iopub.execute_input":"2025-05-09T08:59:25.758778Z","iopub.status.idle":"2025-05-09T08:59:25.772816Z","shell.execute_reply.started":"2025-05-09T08:59:25.758751Z","shell.execute_reply":"2025-05-09T08:59:25.772066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv['real_width'] = np.zeros(len(df_cropped_csv))\ndf_cropped_csv['real_height'] = np.zeros(len(df_cropped_csv))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:07:44.977164Z","iopub.execute_input":"2025-05-09T18:07:44.977483Z","iopub.status.idle":"2025-05-09T18:07:44.983102Z","shell.execute_reply.started":"2025-05-09T18:07:44.977459Z","shell.execute_reply":"2025-05-09T18:07:44.982268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_new_prompt.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:03.07226Z","iopub.execute_input":"2025-05-09T18:08:03.072598Z","iopub.status.idle":"2025-05-09T18:08:03.081841Z","shell.execute_reply.started":"2025-05-09T18:08:03.07257Z","shell.execute_reply":"2025-05-09T18:08:03.080909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idd in df_cropped_csv['image_id']:\n    match = df_with_new_prompt[df_with_new_prompt['Id'] == idd]\n    if not match.empty:\n        df_cropped_csv.loc[df_cropped_csv['image_id'] == idd, 'real_width'] = match['width'].values[0]\n        df_cropped_csv.loc[df_cropped_csv['image_id'] == idd, 'real_height'] = match['height'].values[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:03.521591Z","iopub.execute_input":"2025-05-09T18:08:03.521799Z","iopub.status.idle":"2025-05-09T18:08:30.946442Z","shell.execute_reply.started":"2025-05-09T18:08:03.521782Z","shell.execute_reply":"2025-05-09T18:08:30.945462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_cropped_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:30.94761Z","iopub.execute_input":"2025-05-09T18:08:30.947874Z","iopub.status.idle":"2025-05-09T18:08:30.952766Z","shell.execute_reply.started":"2025-05-09T18:08:30.947853Z","shell.execute_reply":"2025-05-09T18:08:30.951801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(image_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:43.030492Z","iopub.execute_input":"2025-05-09T18:08:43.030805Z","iopub.status.idle":"2025-05-09T18:08:43.035766Z","shell.execute_reply.started":"2025-05-09T18:08:43.030779Z","shell.execute_reply":"2025-05-09T18:08:43.034926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:45.384733Z","iopub.execute_input":"2025-05-09T18:08:45.385123Z","iopub.status.idle":"2025-05-09T18:08:45.409682Z","shell.execute_reply.started":"2025-05-09T18:08:45.385092Z","shell.execute_reply":"2025-05-09T18:08:45.40882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv[['real_width', 'real_height']].isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:51.216746Z","iopub.execute_input":"2025-05-09T18:08:51.217067Z","iopub.status.idle":"2025-05-09T18:08:51.224595Z","shell.execute_reply.started":"2025-05-09T18:08:51.217033Z","shell.execute_reply":"2025-05-09T18:08:51.22388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_path = '/kaggle/input/cropped-images-file'\nmatched_files = [name for name in image_files if '32169' in name]\nimg_path = os.path.join(full_path, matched_files[0])\nimg_arr = cv2.imread(img_path)\nplt.imshow(img_arr)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:08:59.669464Z","iopub.execute_input":"2025-05-09T18:08:59.669774Z","iopub.status.idle":"2025-05-09T18:08:59.771418Z","shell.execute_reply.started":"2025-05-09T18:08:59.66975Z","shell.execute_reply":"2025-05-09T18:08:59.770669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_empty = df_cropped_csv[df_cropped_csv['real_width'].isna()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:34.618139Z","iopub.execute_input":"2025-05-09T18:09:34.618489Z","iopub.status.idle":"2025-05-09T18:09:34.623871Z","shell.execute_reply.started":"2025-05-09T18:09:34.618462Z","shell.execute_reply":"2025-05-09T18:09:34.62314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_empty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:36.419663Z","iopub.execute_input":"2025-05-09T18:09:36.419954Z","iopub.status.idle":"2025-05-09T18:09:36.441471Z","shell.execute_reply.started":"2025-05-09T18:09:36.419933Z","shell.execute_reply":"2025-05-09T18:09:36.44058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_empty[df_empty['real_height']==26])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:39.054389Z","iopub.execute_input":"2025-05-09T18:09:39.054672Z","iopub.status.idle":"2025-05-09T18:09:39.06085Z","shell.execute_reply.started":"2025-05-09T18:09:39.054652Z","shell.execute_reply":"2025-05-09T18:09:39.059951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv.loc[df_cropped_csv['real_height']== 21.5, 'real_width']= 6.5\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 12.0, 'real_width']= 4\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 30.0, 'real_width']= 10\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 24.0, 'real_width']= 6\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 22.0, 'real_width']= 18\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 8.5, 'real_width']= 1.5\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 23.0, 'real_width']= 1\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 11, 'real_width']= 12\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 27.0, 'real_width']= 0\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 140.0, 'real_width']= 0\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 9.0, 'real_width']= 1\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 13, 'real_width']= 6\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 27.0, 'real_width']= 0\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 26.0, 'real_width']= 2\n\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 4, 'real_width']= 4\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 4, 'real_height']= 19\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 18.9, 'real_width']= 0\ndf_cropped_csv.loc[df_cropped_csv['real_height']== 28.0, 'real_width']= 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:39.932369Z","iopub.execute_input":"2025-05-09T18:09:39.932698Z","iopub.status.idle":"2025-05-09T18:09:39.949222Z","shell.execute_reply.started":"2025-05-09T18:09:39.932676Z","shell.execute_reply":"2025-05-09T18:09:39.948569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv = df_cropped_csv.dropna(subset=['real_width'])\ndf_cropped_csv = df_cropped_csv[df_cropped_csv['real_width'] != 0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:42.168021Z","iopub.execute_input":"2025-05-09T18:09:42.168326Z","iopub.status.idle":"2025-05-09T18:09:42.178619Z","shell.execute_reply.started":"2025-05-09T18:09:42.168301Z","shell.execute_reply":"2025-05-09T18:09:42.177912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv = df_cropped_csv.dropna(subset=['real_height'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:43.886084Z","iopub.execute_input":"2025-05-09T18:09:43.88636Z","iopub.status.idle":"2025-05-09T18:09:43.893425Z","shell.execute_reply.started":"2025-05-09T18:09:43.886339Z","shell.execute_reply":"2025-05-09T18:09:43.892438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:46.344163Z","iopub.execute_input":"2025-05-09T18:09:46.34453Z","iopub.status.idle":"2025-05-09T18:09:46.353042Z","shell.execute_reply.started":"2025-05-09T18:09:46.344501Z","shell.execute_reply":"2025-05-09T18:09:46.352324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncsv_features_scaled = scaler.fit_transform(\n    df_cropped_csv.drop(['real_width', 'real_height', 'image_id'], axis=1)\n)\n\ndf_cropped_csv_scaled = df_cropped_csv.copy()\ndf_cropped_csv_scaled.loc[:, df_cropped_csv_scaled.columns.difference(['real_width', 'real_height', 'image_id'])] = csv_features_scaled\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:50.84862Z","iopub.execute_input":"2025-05-09T18:09:50.848903Z","iopub.status.idle":"2025-05-09T18:09:51.410511Z","shell.execute_reply.started":"2025-05-09T18:09:50.848883Z","shell.execute_reply":"2025-05-09T18:09:51.409817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cropped_csv_scaled[df_cropped_csv_scaled['real_width']==40]['real_height'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:09:51.502353Z","iopub.execute_input":"2025-05-09T18:09:51.502793Z","iopub.status.idle":"2025-05-09T18:09:51.50988Z","shell.execute_reply.started":"2025-05-09T18:09:51.502769Z","shell.execute_reply":"2025-05-09T18:09:51.509047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"this codes make it faster","metadata":{}},{"cell_type":"code","source":"#df_merged = df_cropped_csv.merge(\n#    df_with_new_prompt[['Id', 'width', 'height']],\n#    left_on='image_id',\n#    right_on='Id',\n#    how='left'\n#)\n#\n## Rename to match your columns:\n#df_merged.rename(columns={'width': 'real_width', 'height': 'real_height'}, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:21:05.886846Z","iopub.execute_input":"2025-05-09T15:21:05.887195Z","iopub.status.idle":"2025-05-09T15:21:05.890719Z","shell.execute_reply.started":"2025-05-09T15:21:05.887171Z","shell.execute_reply":"2025-05-09T15:21:05.889683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  What We’re Building:","metadata":{}},{"cell_type":"markdown","source":"            [Image]            [CSV features]\n               │                     │\n        Swin Transformer\n            backbone          Tabular Features (MLP)\n               │                     │\n          Image Vector          CSV Vector\n               └──────────────┬───────────────┘\n                              │\n                    Fully Connected Layers\n                              │\n                 Output: [real_width, real_height]\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport glob\n\nclass ImageCSVRegressionDataset(Dataset):\n    def __init__(self, csv_file, image_dir, transform=None):\n        self.df = csv_file\n        self.image_dir = image_dir\n        self.transform = transform\n        \n        # Drop target columns to use as CSV features\n        self.csv_features = self.df.drop(['real_width', 'real_height'], axis=1)\n        \n        # Extract labels\n        self.labels = self.df[['real_width', 'real_height']].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_id = str(int(self.df.iloc[idx]['image_id']))\n        all_files = os.listdir(self.image_dir)\n        matched_files = [f for f in all_files if f.endswith(f\"{img_id}.jpg\")]\n        if not matched_files:\n            raise FileNotFoundError(f\"No image found containing ID {img_id} in {self.image_dir}\")\n            \n        img_path = os.path.join(self.image_dir, matched_files[0])\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n\n        # CSV features (without image_id, remove targets too)\n        csv_row = self.csv_features.iloc[idx]\n        \n        # Optionally drop columns you don't need like image_id, image_width/height\n        csv_row = csv_row.drop(['image_id'])  # Drop more if needed\n        \n        csv_tensor = torch.tensor(csv_row.values, dtype=torch.float32)\n        \n        # Labels\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        \n        return image, csv_tensor, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:15.982812Z","iopub.execute_input":"2025-05-09T18:10:15.983123Z","iopub.status.idle":"2025-05-09T18:10:15.990227Z","shell.execute_reply.started":"2025-05-09T18:10:15.983101Z","shell.execute_reply":"2025-05-09T18:10:15.989287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:17.929807Z","iopub.execute_input":"2025-05-09T18:10:17.930084Z","iopub.status.idle":"2025-05-09T18:10:17.934339Z","shell.execute_reply.started":"2025-05-09T18:10:17.930063Z","shell.execute_reply":"2025-05-09T18:10:17.93345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install timm","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:18.49028Z","iopub.execute_input":"2025-05-09T18:10:18.490627Z","iopub.status.idle":"2025-05-09T18:10:18.49435Z","shell.execute_reply.started":"2025-05-09T18:10:18.490603Z","shell.execute_reply":"2025-05-09T18:10:18.493319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\nimport torch.nn as nn\n\nclass SwinCSVRegressor(nn.Module):\n    def __init__(self, num_csv_features):\n        super(SwinCSVRegressor, self).__init__()\n        \n        # Swin Transformer backbone\n        self.swin = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=0)\n\n        swin_out_dim = self.swin.num_features # will be 768\n\n        # CSV MLP\n        self.csv_mlp = nn.Sequential(\n            nn.Linear(num_csv_features, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            #nn.ReLU(), \n        )\n\n        # Combined regressor\n        self.regressor = nn.Sequential(\n            nn.Linear(swin_out_dim + 32, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2),  # Output: [real_width, real_height]\n            #nn.ReLU()  # ✅ This ensures output >= 0\n        )\n\n    def forward(self, image, csv_features):\n        img_features = self.swin(image)  # shape: [batch_size, 768]\n        \n        if len(img_features.shape) == 4:\n            img_features = img_features.mean(dim=[2, 3])\n        csv_features = self.csv_mlp(csv_features)\n        combined = torch.cat((img_features, csv_features), dim=1)\n        output = self.regressor(combined)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:18.926976Z","iopub.execute_input":"2025-05-09T18:10:18.927287Z","iopub.status.idle":"2025-05-09T18:10:22.372734Z","shell.execute_reply.started":"2025-05-09T18:10:18.927263Z","shell.execute_reply":"2025-05-09T18:10:22.37179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nTARGET_SCALE = 180.0 \n\ndf_cropped_csv_scaled['real_width'] = df_cropped_csv_scaled['real_width'] / TARGET_SCALE\ndf_cropped_csv_scaled['real_height'] = df_cropped_csv_scaled['real_height'] / TARGET_SCALE\n\ncsv_file =  df_cropped_csv_scaled\n\n\nimage_dir = '/kaggle/input/cropped-images-file'\n\ndataset = ImageCSVRegressionDataset(\n    csv_file=csv_file,\n    image_dir=image_dir,\n    transform=transform #this only applies to images\n)\n\n\n# Count CSV feature columns (excluding 'image_id', 'real_width', 'real_height')\nnum_csv_features = dataset.csv_features.drop(['image_id'], axis=1).shape[1]\n\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:22.37378Z","iopub.execute_input":"2025-05-09T18:10:22.374026Z","iopub.status.idle":"2025-05-09T18:10:22.384978Z","shell.execute_reply.started":"2025-05-09T18:10:22.374004Z","shell.execute_reply":"2025-05-09T18:10:22.384325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SwinCSVRegressor(num_csv_features=num_csv_features).to(device)\n#model = CNNCSVRegressor(num_csv_features=num_csv_features).to(device)\n\n# Swin modelini freeze etmek:\n#for param in model.swin.parameters():\n #   param.requires_grad = False\n\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:23.203801Z","iopub.execute_input":"2025-05-09T18:10:23.204095Z","iopub.status.idle":"2025-05-09T18:10:26.691869Z","shell.execute_reply.started":"2025-05-09T18:10:23.204063Z","shell.execute_reply":"2025-05-09T18:10:26.691029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 1\nmae_criterion = nn.L1Loss()\n\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    running_mae = 0.0  # ✅ Track MAE too\n    \n    for images, csv_feats, targets in dataloader:\n        images = images.to(device)\n        csv_feats = csv_feats.to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, csv_feats)\n\n            \n        loss = criterion(outputs, targets)\n        mae = mae_criterion(outputs, targets)  # ✅ new\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        running_mae += mae.item() * images.size(0)  # ✅ accumulate MAE\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_mae = running_mae / len(dataloader.dataset)  # ✅ average MAE\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Avg CM Error: {epoch_mae * 180:.2f}\")\n    \n    torch.cuda.empty_cache()\n\n\nmodel.eval()\nwith torch.no_grad():\n    for images, csv_feats, targets in dataloader:\n        images = images.to(device)\n        csv_feats = csv_feats.to(device)\n        targets = targets.to(device)\n        \n        outputs = model(images, csv_feats)\n\n        # ✅ Burada normalize edilmiş değerleri geri çeviriyoruz:\n        predictions = outputs.detach().cpu().numpy() * TARGET_SCALE\n        true_values = targets.detach().cpu().numpy() * TARGET_SCALE\n\n        # Örnek ilk 5 taneyi yaz\n        for i in range(5):\n            print(f\"Pred: {predictions[i]}, True: {true_values[i]}\")\n        \n        break  # sadece ilk batch bakıyoruz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:55:00.257164Z","iopub.execute_input":"2025-05-09T15:55:00.25747Z","iopub.status.idle":"2025-05-09T16:07:39.279468Z","shell.execute_reply.started":"2025-05-09T15:55:00.257448Z","shell.execute_reply":"2025-05-09T16:07:39.27867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### when i normalized the csv file, 0.0552 x 180 = 9.936 i got\n##### Epoch 1/1, Loss: 0.0080, Avg CM Error: 0.0552","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"with negatives values and without transforms.ColorJitter(brightness=0.2, contrast=0.2)\n##### Epoch 1/10, Loss: 325.3611, Avg CM Error: 11.5749","metadata":{}},{"cell_type":"markdown","source":"withoutd negative predicts and when i used transforms.ColorJitter(brightness=0.2, contrast=0.2) i got:\n##### Epoch 1/10, Loss: 432.4973, Avg CM Error: 13.590","metadata":{}},{"cell_type":"markdown","source":"without colorjitter and after i elimeted the negative predict i got:\n##### Epoch 1/10, Loss: 434.5121, Avg CM Error: 13.6534","metadata":{}},{"cell_type":"markdown","source":"#when i use freezeing with\n\n Swin modelini freeze etmek:\n \n#for param in model.swin.parameters():\n   param.requires_grad = False\n   \n######  Epoch 1/10, Loss: 573.6619, Avg CM Error: 16.2814\n######  Epoch 2/10, Loss: 553.4907, Avg CM Error: 16.0320\n######  Epoch 3/10, Loss: 547.3807, Avg CM Error: 15.8590\n######  Epoch 4/10, Loss: 542.4806, Avg CM Error: 15.7391\n######  Epoch 5/10, Loss: 539.2575, Avg CM Error: 15.6509\n######  Epoch 6/10, Loss: 537.4765, Avg CM Error: 15.6042\n######  Epoch 7/10, Loss: 536.7442, Avg CM Error: 15.5880\n######  Epoch 8/10, Loss: 535.7697, Avg CM Error: 15.5634\n\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# CSV dosyanı ikiye bölüyoruz\ntrain_df, test_df = train_test_split(df_cropped_csv_scaled, test_size=0.2, random_state=42)\n\n# Train dataset\ntrain_dataset = ImageCSVRegressionDataset(\n    csv_file=train_df,\n    image_dir=image_dir,\n    transform=transform\n)\n\n# Test dataset (transform aynısı olabilir)\ntest_dataset = ImageCSVRegressionDataset(\n    csv_file=test_df,\n    image_dir=image_dir,\n    transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:10:34.008981Z","iopub.execute_input":"2025-05-09T18:10:34.009277Z","iopub.status.idle":"2025-05-09T18:10:34.101643Z","shell.execute_reply.started":"2025-05-09T18:10:34.009254Z","shell.execute_reply":"2025-05-09T18:10:34.100948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"class CNNCSVRegressor(nn.Module):\n    def __init__(self, num_csv_features):\n        super(CNNCSVRegressor, self).__init__()\n        \n        # CNN backbone\n        self.cnn = timm.create_model('resnet50', pretrained=True, num_classes=0)\n\n        cnn_out_dim = self.cnn.num_features  # ResNet50 için 2048\n\n        # CSV MLP\n        self.csv_mlp = nn.Sequential(\n            nn.Linear(num_csv_features, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n        )\n\n        # Combined regressor\n        self.regressor = nn.Sequential(\n            nn.Linear(cnn_out_dim + 32, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2),  # Output: [real_width, real_height]\n        )\n\n    def forward(self, image, csv_features):\n        img_features = self.cnn(image)  # ResNet otomatik flatten yapar\n\n        csv_features = self.csv_mlp(csv_features)\n        combined = torch.cat((img_features, csv_features), dim=1)\n        output = self.regressor(combined)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:11:35.243646Z","iopub.execute_input":"2025-05-09T18:11:35.243929Z","iopub.status.idle":"2025-05-09T18:11:35.249541Z","shell.execute_reply.started":"2025-05-09T18:11:35.243908Z","shell.execute_reply":"2025-05-09T18:11:35.24858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNNCSVRegressor(num_csv_features=num_csv_features).to(device)\n#model = CNNCSVRegressor(num_csv_features=num_csv_features).to(device)\n\n# Swin modelini freeze etmek:\n#for param in model.swin.parameters():\n #   param.requires_grad = False\n\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:11:37.842815Z","iopub.execute_input":"2025-05-09T18:11:37.843107Z","iopub.status.idle":"2025-05-09T18:11:38.713137Z","shell.execute_reply.started":"2025-05-09T18:11:37.843085Z","shell.execute_reply":"2025-05-09T18:11:38.712278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 50\npatience = 3  # Kaç epoch boyunca gelişme olmazsa duracak\nbest_loss = float('inf')\ntrigger_times = 0\n\nmae_criterion = nn.L1Loss()\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    running_mae = 0.0\n    \n    for images, csv_feats, targets in train_loader:\n        images = images.to(device)\n        csv_feats = csv_feats.to(device)\n        targets = targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, csv_feats)\n            \n        loss = criterion(outputs, targets)\n        mae = mae_criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        running_mae += mae.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_mae = running_mae / len(train_loader.dataset)\n    epoch_cm_error = epoch_mae * TARGET_SCALE\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Avg CM Error: {epoch_cm_error:.2f} cm\")\n\n    # ✅ Validation testi (test_loader ile)\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, csv_feats, targets in test_loader:\n            images = images.to(device)\n            csv_feats = csv_feats.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images, csv_feats)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * images.size(0)\n\n    val_loss /= len(test_loader.dataset)\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # ✅ Early stopping kontrolü\n    if val_loss < best_loss:\n        best_loss = val_loss\n        trigger_times = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(\" Yeni en iyi model kaydedildi.\")\n    else:\n        trigger_times += 1\n        print(f\"⚠️ Gelişme yok: {trigger_times}/{patience}\")\n        if trigger_times >= patience:\n            print(\"⛔ Early stopping triggered!\")\n            break\n\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:51:30.563045Z","iopub.execute_input":"2025-05-09T18:51:30.563354Z","iopub.status.idle":"2025-05-09T19:31:44.863492Z","shell.execute_reply.started":"2025-05-09T18:51:30.563331Z","shell.execute_reply":"2025-05-09T19:31:44.862567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# En iyi modeli yükle (örneğin: 'best_model.pth')\nmodel.load_state_dict(torch.load('best_model.pth'))\n\nmodel.eval()\nwith torch.no_grad():\n    for images, csv_feats, targets in test_loader:\n        images = images.to(device)\n        csv_feats = csv_feats.to(device)\n        targets = targets.to(device)\n        \n        outputs = model(images, csv_feats)\n        predictions = torch.relu(outputs).detach().cpu().numpy() * TARGET_SCALE\n        true_values = targets.detach().cpu().numpy() * TARGET_SCALE\n\n        # Örnek ilk 5 taneyi yaz\n        for i in range(5):\n            print(f\"Pred: {predictions[i]}, True: {true_values[i]}\")\n        \n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:43:42.520139Z","iopub.execute_input":"2025-05-09T19:43:42.520581Z","iopub.status.idle":"2025-05-09T19:43:43.160466Z","shell.execute_reply.started":"2025-05-09T19:43:42.520547Z","shell.execute_reply":"2025-05-09T19:43:43.159586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation setindeki tüm hata değerlerini toplayacağız\ntotal_mae_cm = 0.0\ntotal_samples = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, csv_feats, targets in test_loader:\n        images = images.to(device)\n        csv_feats = csv_feats.to(device)\n        targets = targets.to(device)\n\n        outputs = model(images, csv_feats)\n        predictions = torch.relu(outputs)\n\n        # MAE'yi hesapla (normalize edilmiş)\n        mae = torch.abs(predictions - targets).mean(dim=1)  # her sample için ortalama hata\n        total_mae_cm += mae.sum().item() * TARGET_SCALE  # tüm batch'teki hataları topluyoruz\n        total_samples += images.size(0)\n\n# Genel Validation CM hatasını yazdır\navg_cm_error_val = total_mae_cm / total_samples\nprint(f\"Validation Set Avg CM Error: {avg_cm_error_val:.2f} cm\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:45:48.407973Z","iopub.execute_input":"2025-05-09T19:45:48.408301Z","iopub.status.idle":"2025-05-09T19:46:56.964034Z","shell.execute_reply.started":"2025-05-09T19:45:48.408275Z","shell.execute_reply":"2025-05-09T19:46:56.963264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}